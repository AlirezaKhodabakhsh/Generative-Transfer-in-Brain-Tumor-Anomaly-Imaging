{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Libs"
   ],
   "metadata": {
    "collapsed": false,
    "id": "iH78UqN5TXOR"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10, CIFAR100\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from torch import optim\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "z5yRfPbnTXOX"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Helper"
   ],
   "metadata": {
    "collapsed": false,
    "id": "eFMELjrMTXOZ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, data, targets, transform, target_transform=None):\n",
    "        self.targets = targets\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = Image.fromarray(self.data[index])\n",
    "        if self.transform is not None:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        y = self.targets[index]\n",
    "        if self.target_transform is not None:\n",
    "            y = self.target_transform(y)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def sparse_to_coarse(targets):\n",
    "    coarse_labels = np.array([4, 1, 14, 8, 0, 6, 7, 7, 18, 3,\n",
    "                              3, 14, 9, 18, 7, 11, 3, 9, 7, 11,\n",
    "                              6, 11, 5, 10, 7, 6, 13, 15, 3, 15,\n",
    "                              0, 11, 1, 10, 12, 14, 16, 9, 11, 5,\n",
    "                              5, 19, 8, 8, 15, 13, 14, 17, 18, 10,\n",
    "                              16, 4, 17, 4, 2, 0, 17, 4, 18, 17,\n",
    "                              10, 3, 2, 12, 12, 16, 12, 1, 9, 19,\n",
    "                              2, 10, 0, 1, 16, 12, 9, 13, 15, 13,\n",
    "                              16, 19, 2, 4, 6, 19, 5, 5, 8, 19,\n",
    "                              18, 1, 2, 15, 6, 0, 17, 8, 14, 13])\n",
    "    return coarse_labels[targets]"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "TcwhlO03TXOa"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train"
   ],
   "metadata": {
    "collapsed": false,
    "id": "gJxWdx9uTXOb"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "id": "MHdrXvG1TXOb"
   },
   "outputs": [],
   "source": [
    "def get_train_transforms():\n",
    "    transform_train = transforms.Compose([transforms.Resize(32),\n",
    "                                          transforms.RandomCrop(32),\n",
    "                                          transforms.RandomHorizontalFlip(),\n",
    "                                          transforms.ToTensor(),\n",
    "                                          transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                                               std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    return transform_train\n",
    "\n",
    "\n",
    "def get_nomral_dataset_train(dataset_name, label, data_path, download):\n",
    "    if dataset_name == 'cifar10':\n",
    "        normal_train_ds = CIFAR10(data_path, train=True, download=download)\n",
    "    elif dataset_name == 'cifar100':\n",
    "        normal_train_ds = CIFAR100(data_path, train=True, download=download)\n",
    "        normal_train_ds.targets = sparse_to_coarse(normal_train_ds.targets)\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    normal_data = normal_train_ds.data[np.array(normal_train_ds.targets) == label]\n",
    "    normal_train_ds = BaseDataset(normal_data, [0] * len(normal_data), get_train_transforms())\n",
    "    return normal_train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "normal_train_ds = get_nomral_dataset_train('cifar10', 0, './CIFAR10', True)\n",
    "\n",
    "normal_train_loader = DataLoader(normal_train_ds, batch_size=100, shuffle=True)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "5wD3OOYYTXOc"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "DGlT_K2kTXOd"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def get_test_transforms():\n",
    "    transform_test = transforms.Compose([transforms.Resize(32),\n",
    "                                         transforms.CenterCrop(32),\n",
    "                                         transforms.ToTensor(),\n",
    "                                        transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                                             std=[0.5, 0.5, 0.5])])\n",
    "    return transform_test\n",
    "\n",
    "\n",
    "\n",
    "def get_dataset_test(normal_label, ano_label, data_path, download):\n",
    "    test_cifar10 = CIFAR10(data_path, train=False, download=download)\n",
    "    normal_test_data = test_cifar10.data[np.array(test_cifar10.targets) == normal_label]\n",
    "    normal_test_ds = BaseDataset(normal_test_data, [0] * len(normal_test_data), get_test_transforms())\n",
    "\n",
    "    test_cifar10 = CIFAR10(data_path, train=False, download=download)\n",
    "    ano_test_data = test_cifar10.data[np.array(test_cifar10.targets) == ano_label]\n",
    "    ano_test_ds = BaseDataset(ano_test_data, [1] * len(ano_test_data), get_test_transforms())\n",
    "\n",
    "\n",
    "    return normal_test_ds, ano_test_ds"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "-kkQet83TXOd"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "normal_test_ds, ano_test_ds = get_dataset_test(0, 1, './CIFAR10', True)\n",
    "\n",
    "normal_test_loader = DataLoader(normal_test_ds, batch_size=100, shuffle=True)\n",
    "ano_test_loader = DataLoader(ano_test_ds, batch_size=100, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TEST Lib rary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from customdataset import get_nomral_dataset_train, get_dataset_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "normal_test_ds, ano_test_ds = get_dataset_test(0, 1, './CIFAR10', True)\n",
    "normal_train_ds = get_nomral_dataset_train('cifar10', 0, './CIFAR10', True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TSNE"
   ],
   "metadata": {
    "collapsed": false,
    "id": "IMuRN9HTTXOe"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def get_2d_tsne(x):\n",
    "    tsne = TSNE(n_components=2, random_state=0,  learning_rate='auto', init='random', perplexity=3)\n",
    "    x_2d = tsne.fit_transform(x)\n",
    "\n",
    "    return x_2d"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# UNet"
   ],
   "metadata": {
    "collapsed": false,
    "id": "Zd7tMUvPTXOg"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels=3, out_channels=3, init_features=32):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        img_size = 32\n",
    "        features = init_features\n",
    "        self.encoder1 = UNet._block(in_channels, features, name=\"enc1\")\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder2 = UNet._block(features, features * 2, name=\"enc2\")\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder3 = UNet._block(features * 2, features * 4, name=\"enc3\")\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder4 = UNet._block(features * 4, features * 8, name=\"enc4\")\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.bottleneck = UNet._block(features * 8, features * 16, name=\"bottleneck\")\n",
    "\n",
    "        dim = 16*features * (img_size**2)/(2**8)\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(int(dim/1), int(dim/2)),\n",
    "            nn.BatchNorm1d(int(dim/2)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(int(dim/2), int(dim/4)),\n",
    "            nn.BatchNorm1d(int(dim/4)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(int(dim/4), int(dim/2)),\n",
    "            nn.BatchNorm1d(int(dim/2)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.fc4 = nn.Sequential(\n",
    "            nn.Linear(int(dim/2), int(dim/1)),\n",
    "            nn.BatchNorm1d(int(dim/1)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(\n",
    "            features * 16, features * 8, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder4 = UNet._block((features * 8) * 2, features * 8, name=\"dec4\")\n",
    "        self.upconv3 = nn.ConvTranspose2d(\n",
    "            features * 8, features * 4, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder3 = UNet._block((features * 4) * 2, features * 4, name=\"dec3\")\n",
    "        self.upconv2 = nn.ConvTranspose2d(\n",
    "            features * 4, features * 2, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder2 = UNet._block((features * 2) * 2, features * 2, name=\"dec2\")\n",
    "        self.upconv1 = nn.ConvTranspose2d(\n",
    "            features * 2, features, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder1 = UNet._block(features * 2, features, name=\"dec1\")\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=features, out_channels=out_channels, kernel_size=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool1(enc1))\n",
    "        enc3 = self.encoder3(self.pool2(enc2))\n",
    "        enc4 = self.encoder4(self.pool3(enc3))\n",
    "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
    "\n",
    "        fc1 = self.fc1(bottleneck.flatten(1))\n",
    "        fc2 = self.fc2(fc1)\n",
    "\n",
    "        self.compress_features = fc2\n",
    "\n",
    "        if y is not None:\n",
    "          fc3 = self.fc3(y)\n",
    "        else:\n",
    "          fc3 = self.fc3(self.compress_features)\n",
    "\n",
    "\n",
    "        fc4 = self.fc4(fc3)\n",
    "\n",
    "        bottleneck = fc4.view(bottleneck.shape)\n",
    "        dec4 = self.upconv4(bottleneck)\n",
    "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
    "        dec4 = self.decoder4(dec4)\n",
    "        dec3 = self.upconv3(dec4)\n",
    "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "        return torch.tanh(self.conv(dec1))\n",
    "\n",
    "    @staticmethod\n",
    "    def _block(in_channels, features, name):\n",
    "        return nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\n",
    "                        name + \"conv1\",\n",
    "                        nn.Conv2d(\n",
    "                            in_channels=in_channels,\n",
    "                            out_channels=features,\n",
    "                            kernel_size=3,\n",
    "                            padding=1,\n",
    "                            bias=False,\n",
    "                        ),\n",
    "                    ),\n",
    "                    (name + \"norm1\", nn.BatchNorm2d(num_features=features)),\n",
    "                    (name + \"relu1\", nn.ReLU(inplace=True)),\n",
    "                    (\n",
    "                        name + \"conv2\",\n",
    "                        nn.Conv2d(\n",
    "                            in_channels=features,\n",
    "                            out_channels=features,\n",
    "                            kernel_size=3,\n",
    "                            padding=1,\n",
    "                            bias=False,\n",
    "                        ),\n",
    "                    ),\n",
    "                    (name + \"norm2\", nn.BatchNorm2d(num_features=features)),\n",
    "                    (name + \"relu2\", nn.ReLU(inplace=True)),\n",
    "                ]\n",
    "            )\n",
    "        )"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "twldDRUfTXOg"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "O2vJ1NShTXOh"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "normal_train_ds = get_nomral_dataset_train('cifar10', 0, './CIFAR10', False)\n",
    "\n",
    "normal_train_loader = DataLoader(normal_train_ds, batch_size=100, shuffle=True)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "4AALSEa9TXOh"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "model = UNet().to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "critic = nn.MSELoss()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "XyHtcQNPTXOi"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "normal_test_ds, ano_test_ds = get_dataset_test(0, 1, './CIFAR10', False)\n",
    "\n",
    "normal_test_loader = DataLoader(normal_test_ds, batch_size=normal_test_ds.__len__(), shuffle=True)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "ZZbbs4Q1TXOi"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "6ERdOO4wTXOi"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for x, _ in normal_test_loader:\n",
    "    x_test = x\n",
    "    x_test = x_test.to(device)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Fy9ooF4ITXOi"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss_min = np.Inf\n",
    "model.train()\n",
    "for epoch in range(1000):\n",
    "    torch.cuda.empty_cache()\n",
    "    for x, _ in normal_train_loader:\n",
    "        torch.cuda.empty_cache()\n",
    "        x = x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        x_hat = model(x)\n",
    "        loss = critic(x_hat, x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    x_hat = model(x_test)\n",
    "    loss = critic(x_hat, x_test)\n",
    "    print(f\"Epoch{epoch} : Loss = {loss}\")"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Y-bZ47TZTXOj"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TSNE"
   ],
   "metadata": {
    "collapsed": false,
    "id": "96aDc3JPTXOj"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_hat = model(x_test)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "52QBBrtFTXOj"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x1 = model.compress_features.flatten(1)\n",
    "x2 = x_test.flatten(1)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "MKXwDg-qTXOj"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x1 = x1.detach().cpu()\n",
    "x2 = x2.detach().cpu()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "0hVew2vQTXOj"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_2d_tsne(x):\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    x_2d = tsne.fit_transform(x)\n",
    "\n",
    "    return x_2d"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "8bKzyhUATXOj"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(get_2d_tsne(x1)[:,0], get_2d_tsne(x1)[:,1], label='Compress')\n",
    "plt.scatter(get_2d_tsne(x2)[:,0], get_2d_tsne(x2)[:,1], label='Raw')\n",
    "plt.legend()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "wo9K_FpPTXOk"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(get_2d_tsne(x1)[:,0], get_2d_tsne(x1)[:,1], label='Compress')"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "_VS8UhC-TXOk"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# new"
   ],
   "metadata": {
    "id": "9FI44AhJW0up"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "normal_test_ds, ano_test_ds = get_dataset_test(0, 2, './CIFAR10', True)\n",
    "\n",
    "normal_test_loader = DataLoader(normal_test_ds, batch_size=1000, shuffle=True)\n",
    "ano_test_loader = DataLoader(ano_test_ds, batch_size=1000, shuffle=True)"
   ],
   "metadata": {
    "id": "Gp0cfEsvW12a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for x,_ in ano_test_loader:\n",
    "  x_ano_1=x\n",
    "  break"
   ],
   "metadata": {
    "id": "zlZ-xF5TW7-T"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for x,_ in normal_test_loader:\n",
    "  x_normal=x\n",
    "  break"
   ],
   "metadata": {
    "id": "_fChY18eXEsJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = model.cpu()\n",
    "model.eval()\n",
    "\n",
    "y_normal = model(x_normal)\n",
    "x_normal_compress = model.compress_features.flatten(1).detach().cpu()\n",
    "\n",
    "y_ano_1 = model(x_ano_1)\n",
    "x_ano_1_compress = model.compress_features.flatten(1).detach().cpu()"
   ],
   "metadata": {
    "id": "UqRilFJ_XK-p"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "x_normal = x_normal.flatten(1)\n",
    "x_ano_1 = x_ano_1.flatten(1)"
   ],
   "metadata": {
    "id": "6uJs3W39XbrR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure()\n",
    "plt.scatter(get_2d_tsne(x_normal_compress)[:,0], get_2d_tsne(x_normal_compress)[:,1], label='Normal')\n",
    "plt.scatter(get_2d_tsne(x_ano_1_compress)[:,0], get_2d_tsne(x_ano_1_compress)[:,1], label='Ano')\n",
    "plt.legend()"
   ],
   "metadata": {
    "id": "UaW1bKCJXYhB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "Cu5W_JFlYKaE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Add noise"
   ],
   "metadata": {
    "id": "2b6IJ9i63Wjy"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "a52HwbXI3YAD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for x,_ in normal_test_loader:\n",
    "  x_normal=x\n",
    "  break"
   ],
   "metadata": {
    "id": "DwHSyWi73YLy"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = model.cpu()\n",
    "model.eval()\n",
    "\n",
    "y_normal = model(x_normal)\n",
    "x_normal_compress = model.compress_features.flatten(1).detach().cpu()\n",
    "\n",
    "x_normal_compress_noise = torch.empty(512).normal_(0,5) + x_normal_compress\n",
    "y_normal_compress_noise = model(x_normal, x_normal_compress_noise)"
   ],
   "metadata": {
    "id": "Q2W3TkPY3YLy"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "x_normal_compress"
   ],
   "metadata": {
    "id": "JRywrFh-70l6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "x_normal_compress_noise"
   ],
   "metadata": {
    "id": "GoVtOhh47zaj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure()\n",
    "plt.scatter(get_2d_tsne(x_normal_compress)[:,0], get_2d_tsne(x_normal_compress)[:,1], label='Normal')\n",
    "plt.scatter(get_2d_tsne(x_normal_compress_noise)[:,0], get_2d_tsne(x_normal_compress_noise)[:,1], label='Ano')\n",
    "plt.legend()"
   ],
   "metadata": {
    "id": "FHo51QAl3YLy"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "y_normal_compress_noise.shape"
   ],
   "metadata": {
    "id": "_oUqwI1y5FJP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "y_normal_compress_noise"
   ],
   "metadata": {
    "id": "ekA6d2o28Glb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "y_normal"
   ],
   "metadata": {
    "id": "e2-em4ZU8HHK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.imshow(y_normal_compress_noise[1].permute(1,2,0).detach().cpu())"
   ],
   "metadata": {
    "id": "a1OW3wqU7UpZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.imshow(y_normal[1].permute(1,2,0).detach().cpu())"
   ],
   "metadata": {
    "id": "aLA0FWIY7dnR"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}