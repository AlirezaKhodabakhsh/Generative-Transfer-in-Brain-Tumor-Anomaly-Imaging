{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Lib"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import math\n",
    "# main libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset\n",
    "import math\n",
    "import torchvision.transforms.functional as TF\n",
    "import cv2\n",
    "from keras.utils import image_dataset_from_directory\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "from torch.nn import init\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "import warnings\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Layers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class PixelNormLayer(nn.Module):\n",
    "    def __init__(self, epsilon=1e-8):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.rsqrt(torch.mean(x ** 2, dim=1, keepdim=True) + self.epsilon)\n",
    "\n",
    "\n",
    "class Upscale2d(nn.Module):\n",
    "    @staticmethod\n",
    "    def upscale2d(x, factor=2, gain=1):\n",
    "        assert x.dim() == 4\n",
    "        if gain != 1:\n",
    "            x = x * gain\n",
    "        if factor != 1:\n",
    "            shape = x.shape\n",
    "            x = x.view(shape[0], shape[1], shape[2], 1, shape[3], 1).expand(-1, -1, -1, factor, -1, factor)\n",
    "            x = x.contiguous().view(shape[0], shape[1], factor * shape[2], factor * shape[3])\n",
    "        return x\n",
    "\n",
    "    def __init__(self, factor=2, gain=1):\n",
    "        super().__init__()\n",
    "        assert isinstance(factor, int) and factor >= 1\n",
    "        self.gain = gain\n",
    "        self.factor = factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.upscale2d(x, factor=self.factor, gain=self.gain)\n",
    "\n",
    "\n",
    "class Downscale2d(nn.Module):\n",
    "    def __init__(self, factor=2, gain=1):\n",
    "        super().__init__()\n",
    "        assert isinstance(factor, int) and factor >= 1\n",
    "        self.factor = factor\n",
    "        self.gain = gain\n",
    "        if factor == 2:\n",
    "            f = [np.sqrt(gain) / factor] * factor\n",
    "            self.blur = BlurLayer(kernel=f, normalize=False, stride=factor)\n",
    "        else:\n",
    "            self.blur = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.dim() == 4\n",
    "        # 2x2, float32 => downscale using _blur2d().\n",
    "        if self.blur is not None and x.dtype == torch.float32:\n",
    "            return self.blur(x)\n",
    "\n",
    "        # Apply gain.\n",
    "        if self.gain != 1:\n",
    "            x = x * self.gain\n",
    "\n",
    "        # No-op => early exit.\n",
    "        if self.factor == 1:\n",
    "            return x\n",
    "\n",
    "        # Large factor => downscale using tf.nn.avg_pool().\n",
    "        # NOTE: Requires tf_config['graph_options.place_pruned_graph']=True to work.\n",
    "        return F.avg_pool2d(x, self.factor)\n",
    "\n",
    "\n",
    "class EqualizedLinear(nn.Module):\n",
    "    \"\"\"Linear layer with equalized learning rate and custom learning rate multiplier.\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, output_size, gain=2 ** 0.5, use_wscale=False, lrmul=1, bias=True):\n",
    "        super().__init__()\n",
    "        he_std = gain * input_size ** (-0.5)  # He init\n",
    "        # Equalized learning rate and custom learning rate multiplier.\n",
    "        if use_wscale:\n",
    "            init_std = 1.0 / lrmul\n",
    "            self.w_mul = he_std * lrmul\n",
    "        else:\n",
    "            init_std = he_std / lrmul\n",
    "            self.w_mul = lrmul\n",
    "        self.weight = torch.nn.Parameter(torch.randn(output_size, input_size) * init_std)\n",
    "        if bias:\n",
    "            self.bias = torch.nn.Parameter(torch.zeros(output_size))\n",
    "            self.b_mul = lrmul\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        bias = self.bias\n",
    "        if bias is not None:\n",
    "            bias = bias * self.b_mul\n",
    "        return F.linear(x, self.weight * self.w_mul, bias)\n",
    "\n",
    "\n",
    "class EqualizedConv2d(nn.Module):\n",
    "    \"\"\"Conv layer with equalized learning rate and custom learning rate multiplier.\"\"\"\n",
    "\n",
    "    def __init__(self, input_channels, output_channels, kernel_size, stride=1, gain=2 ** 0.5, use_wscale=False,\n",
    "                 lrmul=1, bias=True, intermediate=None, upscale=False, downscale=False):\n",
    "        super().__init__()\n",
    "        if upscale:\n",
    "            self.upscale = Upscale2d()\n",
    "        else:\n",
    "            self.upscale = None\n",
    "        if downscale:\n",
    "            self.downscale = Downscale2d()\n",
    "        else:\n",
    "            self.downscale = None\n",
    "        he_std = gain * (input_channels * kernel_size ** 2) ** (-0.5)  # He init\n",
    "        self.kernel_size = kernel_size\n",
    "        if use_wscale:\n",
    "            init_std = 1.0 / lrmul\n",
    "            self.w_mul = he_std * lrmul\n",
    "        else:\n",
    "            init_std = he_std / lrmul\n",
    "            self.w_mul = lrmul\n",
    "        self.weight = torch.nn.Parameter(\n",
    "            torch.randn(output_channels, input_channels, kernel_size, kernel_size) * init_std)\n",
    "        if bias:\n",
    "            self.bias = torch.nn.Parameter(torch.zeros(output_channels))\n",
    "            self.b_mul = lrmul\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.intermediate = intermediate\n",
    "\n",
    "    def forward(self, x):\n",
    "        bias = self.bias\n",
    "        if bias is not None:\n",
    "            bias = bias * self.b_mul\n",
    "\n",
    "        have_convolution = False\n",
    "        if self.upscale is not None and min(x.shape[2:]) * 2 >= 128:\n",
    "            # this is the fused upscale + conv from StyleGAN, sadly this seems incompatible with the non-fused way\n",
    "            # this really needs to be cleaned up and go into the conv...\n",
    "            w = self.weight * self.w_mul\n",
    "            w = w.permute(1, 0, 2, 3)\n",
    "            # probably applying a conv on w would be more efficient. also this quadruples the weight (average)?!\n",
    "            w = F.pad(w, [1, 1, 1, 1])\n",
    "            w = w[:, :, 1:, 1:] + w[:, :, :-1, 1:] + w[:, :, 1:, :-1] + w[:, :, :-1, :-1]\n",
    "            x = F.conv_transpose2d(x, w, stride=2, padding=(w.size(-1) - 1) // 2)\n",
    "            have_convolution = True\n",
    "        elif self.upscale is not None:\n",
    "            x = self.upscale(x)\n",
    "\n",
    "        downscale = self.downscale\n",
    "        intermediate = self.intermediate\n",
    "        if downscale is not None and min(x.shape[2:]) >= 128:\n",
    "            w = self.weight * self.w_mul\n",
    "            w = F.pad(w, [1, 1, 1, 1])\n",
    "            # in contrast to upscale, this is a mean...\n",
    "            w = (w[:, :, 1:, 1:] + w[:, :, :-1, 1:] + w[:, :, 1:, :-1] + w[:, :, :-1, :-1]) * 0.25  # avg_pool?\n",
    "            x = F.conv2d(x, w, stride=2, padding=(w.size(-1) - 1) // 2)\n",
    "            have_convolution = True\n",
    "            downscale = None\n",
    "        elif downscale is not None:\n",
    "            assert intermediate is None\n",
    "            intermediate = downscale\n",
    "\n",
    "        if not have_convolution and intermediate is None:\n",
    "            return F.conv2d(x, self.weight * self.w_mul, bias, padding=self.kernel_size // 2)\n",
    "        elif not have_convolution:\n",
    "            x = F.conv2d(x, self.weight * self.w_mul, None, padding=self.kernel_size // 2)\n",
    "\n",
    "        if intermediate is not None:\n",
    "            x = intermediate(x)\n",
    "\n",
    "        if bias is not None:\n",
    "            x = x + bias.view(1, -1, 1, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class NoiseLayer(nn.Module):\n",
    "    \"\"\"adds noise. noise is per pixel (constant over channels) with per-channel weight\"\"\"\n",
    "\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.zeros(channels))\n",
    "        self.noise = None\n",
    "\n",
    "    def forward(self, x, noise=None):\n",
    "        if noise is None and self.noise is None:\n",
    "            noise = torch.randn(x.size(0), 1, x.size(2), x.size(3), device=x.device, dtype=x.dtype)\n",
    "        elif noise is None:\n",
    "            # here is a little trick: if you get all the noise layers and set each\n",
    "            # modules .noise attribute, you can have pre-defined noise.\n",
    "            # Very useful for analysis\n",
    "            noise = self.noise\n",
    "        x = x + self.weight.view(1, -1, 1, 1) * noise\n",
    "        return x\n",
    "\n",
    "\n",
    "class StyleMod(nn.Module):\n",
    "    def __init__(self, latent_size, channels, use_wscale):\n",
    "        super(StyleMod, self).__init__()\n",
    "        self.lin = EqualizedLinear(latent_size,\n",
    "                                   channels * 2,\n",
    "                                   gain=1.0, use_wscale=use_wscale)\n",
    "\n",
    "    def forward(self, x, latent):\n",
    "        style = self.lin(latent)  # style => [batch_size, n_channels*2]\n",
    "\n",
    "        shape = [-1, 2, x.size(1)] + (x.dim() - 2) * [1]\n",
    "        style = style.view(shape)  # [batch_size, 2, n_channels, ...]\n",
    "        x = x * (style[:, 0] + 1.) + style[:, 1]\n",
    "        return x\n",
    "\n",
    "\n",
    "class LayerEpilogue(nn.Module):\n",
    "    \"\"\"Things to do at the end of each layer.\"\"\"\n",
    "\n",
    "    def __init__(self, channels, dlatent_size, use_wscale,\n",
    "                 use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        if use_noise:\n",
    "            layers.append(('noise', NoiseLayer(channels)))\n",
    "        layers.append(('activation', activation_layer))\n",
    "        if use_pixel_norm:\n",
    "            layers.append(('pixel_norm', PixelNormLayer()))\n",
    "        if use_instance_norm:\n",
    "            layers.append(('instance_norm', nn.InstanceNorm2d(channels)))\n",
    "\n",
    "        self.top_epi = nn.Sequential(OrderedDict(layers))\n",
    "\n",
    "        if use_styles:\n",
    "            self.style_mod = StyleMod(dlatent_size, channels, use_wscale=use_wscale)\n",
    "        else:\n",
    "            self.style_mod = None\n",
    "\n",
    "    def forward(self, x, dlatents_in_slice=None):\n",
    "        x = self.top_epi(x)\n",
    "        if self.style_mod is not None:\n",
    "            x = self.style_mod(x, dlatents_in_slice)\n",
    "        else:\n",
    "            assert dlatents_in_slice is None\n",
    "        return x\n",
    "\n",
    "\n",
    "class BlurLayer(nn.Module):\n",
    "    def __init__(self, kernel=None, normalize=True, flip=False, stride=1):\n",
    "        super(BlurLayer, self).__init__()\n",
    "        if kernel is None:\n",
    "            kernel = [1, 2, 1]\n",
    "        kernel = torch.tensor(kernel, dtype=torch.float32)\n",
    "        kernel = kernel[:, None] * kernel[None, :]\n",
    "        kernel = kernel[None, None]\n",
    "        if normalize:\n",
    "            kernel = kernel / kernel.sum()\n",
    "        if flip:\n",
    "            kernel = kernel[:, :, ::-1, ::-1]\n",
    "        self.register_buffer('kernel', kernel)\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        # expand kernel channels\n",
    "        kernel = self.kernel.expand(x.size(1), -1, -1, -1)\n",
    "        x = F.conv2d(\n",
    "            x,\n",
    "            kernel,\n",
    "            stride=self.stride,\n",
    "            padding=int((self.kernel.size(2) - 1) / 2),\n",
    "            groups=x.size(1)\n",
    "        )\n",
    "        return x\n",
    "\n",
    "\n",
    "class View(nn.Module):\n",
    "    def __init__(self, *shape):\n",
    "        super().__init__()\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), *self.shape)\n",
    "\n",
    "\n",
    "class StddevLayer(nn.Module):\n",
    "    def __init__(self, group_size=4, num_new_features=1):\n",
    "        super().__init__()\n",
    "        self.group_size = group_size\n",
    "        self.num_new_features = num_new_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        group_size = min(self.group_size, b)\n",
    "        y = x.reshape([group_size, -1, self.num_new_features,\n",
    "                       c // self.num_new_features, h, w])\n",
    "        y = y - y.mean(0, keepdim=True)\n",
    "        y = (y ** 2).mean(0, keepdim=True)\n",
    "        y = (y + 1e-8) ** 0.5\n",
    "        y = y.mean([3, 4, 5], keepdim=True).squeeze(3)  # don't keep the meaned-out channels\n",
    "        y = y.expand(group_size, -1, -1, h, w).clone().reshape(b, self.num_new_features, h, w)\n",
    "        z = torch.cat([x, y], dim=1)\n",
    "        return z\n",
    "\n",
    "\n",
    "class Truncation(nn.Module):\n",
    "    def __init__(self, avg_latent, max_layer=8, threshold=0.7, beta=0.995):\n",
    "        super().__init__()\n",
    "        self.max_layer = max_layer\n",
    "        self.threshold = threshold\n",
    "        self.beta = beta\n",
    "        self.register_buffer('avg_latent', avg_latent)\n",
    "\n",
    "    def update(self, last_avg):\n",
    "        self.avg_latent.copy_(self.beta * self.avg_latent + (1. - self.beta) * last_avg)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.dim() == 3\n",
    "        interp = torch.lerp(self.avg_latent, x, self.threshold)\n",
    "        do_trunc = (torch.arange(x.size(1)) < self.max_layer).view(1, -1, 1).to(x.device)\n",
    "        return torch.where(do_trunc, interp, x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Blocks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "class InputBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    The first block (4x4 \"pixels\") doesn't have an input.\n",
    "    The result of the first convolution is just replaced by a (trained) constant.\n",
    "    We call it the InputBlock, the others GSynthesisBlock.\n",
    "    (It might be nicer to do this the other way round,\n",
    "    i.e. have the LayerEpilogue be the Layer and call the conv from that.)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nf, dlatent_size, const_input_layer, gain,\n",
    "                 use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer):\n",
    "        super().__init__()\n",
    "        self.const_input_layer = const_input_layer\n",
    "        self.nf = nf\n",
    "\n",
    "        if self.const_input_layer:\n",
    "            # called 'const' in tf\n",
    "            self.const = nn.Parameter(torch.ones(1, nf, 4, 4))\n",
    "            self.bias = nn.Parameter(torch.ones(nf))\n",
    "        else:\n",
    "            self.dense = EqualizedLinear(dlatent_size, nf * 16, gain=gain / 4,\n",
    "                                         use_wscale=use_wscale)\n",
    "            # tweak gain to match the official implementation of Progressing GAN\n",
    "\n",
    "        self.epi1 = LayerEpilogue(nf, dlatent_size, use_wscale, use_noise, use_pixel_norm, use_instance_norm,\n",
    "                                  use_styles, activation_layer)\n",
    "        self.conv = EqualizedConv2d(nf, nf, 3, gain=gain, use_wscale=use_wscale)\n",
    "        self.epi2 = LayerEpilogue(nf, dlatent_size, use_wscale, use_noise, use_pixel_norm, use_instance_norm,\n",
    "                                  use_styles, activation_layer)\n",
    "\n",
    "    def forward(self, dlatents_in_range):\n",
    "        batch_size = dlatents_in_range.size(0)\n",
    "\n",
    "        if self.const_input_layer:\n",
    "            x = self.const.expand(batch_size, -1, -1, -1)\n",
    "            x = x + self.bias.view(1, -1, 1, 1)\n",
    "        else:\n",
    "            x = self.dense(dlatents_in_range[:, 0]).view(batch_size, self.nf, 4, 4)\n",
    "\n",
    "        x = self.epi1(x, dlatents_in_range[:, 0])\n",
    "        x = self.conv(x)\n",
    "        x = self.epi2(x, dlatents_in_range[:, 1])\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GSynthesisBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, blur_filter, dlatent_size, gain,\n",
    "                 use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer):\n",
    "        # 2**res x 2**res\n",
    "        # res = 3..resolution_log2\n",
    "        super().__init__()\n",
    "\n",
    "        if blur_filter:\n",
    "            blur = BlurLayer(blur_filter)\n",
    "        else:\n",
    "            blur = None\n",
    "\n",
    "        self.conv0_up = EqualizedConv2d(in_channels, out_channels, kernel_size=3, gain=gain, use_wscale=use_wscale,\n",
    "                                        intermediate=blur, upscale=True)\n",
    "        self.epi1 = LayerEpilogue(out_channels, dlatent_size, use_wscale, use_noise, use_pixel_norm, use_instance_norm,\n",
    "                                  use_styles, activation_layer)\n",
    "        self.conv1 = EqualizedConv2d(out_channels, out_channels, kernel_size=3, gain=gain, use_wscale=use_wscale)\n",
    "        self.epi2 = LayerEpilogue(out_channels, dlatent_size, use_wscale, use_noise, use_pixel_norm, use_instance_norm,\n",
    "                                  use_styles, activation_layer)\n",
    "\n",
    "    def forward(self, x, dlatents_in_range):\n",
    "        x = self.conv0_up(x)\n",
    "        x = self.epi1(x, dlatents_in_range[:, 0])\n",
    "        x = self.conv1(x)\n",
    "        x = self.epi2(x, dlatents_in_range[:, 1])\n",
    "        return x\n",
    "\n",
    "\n",
    "class DiscriminatorTop(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 mbstd_group_size,\n",
    "                 mbstd_num_features,\n",
    "                 in_channels,\n",
    "                 intermediate_channels,\n",
    "                 gain, use_wscale,\n",
    "                 activation_layer,\n",
    "                 resolution=4,\n",
    "                 in_channels2=None,\n",
    "                 output_features=1,\n",
    "                 last_gain=1):\n",
    "        \"\"\"\n",
    "        :param mbstd_group_size:\n",
    "        :param mbstd_num_features:\n",
    "        :param in_channels:\n",
    "        :param intermediate_channels:\n",
    "        :param gain:\n",
    "        :param use_wscale:\n",
    "        :param activation_layer:\n",
    "        :param resolution:\n",
    "        :param in_channels2:\n",
    "        :param output_features:\n",
    "        :param last_gain:\n",
    "        \"\"\"\n",
    "\n",
    "        layers = []\n",
    "        if mbstd_group_size > 1:\n",
    "            layers.append(('stddev_layer', StddevLayer(mbstd_group_size, mbstd_num_features)))\n",
    "\n",
    "        if in_channels2 is None:\n",
    "            in_channels2 = in_channels\n",
    "\n",
    "        layers.append(('conv', EqualizedConv2d(in_channels + mbstd_num_features, in_channels2, kernel_size=3,\n",
    "                                               gain=gain, use_wscale=use_wscale)))\n",
    "        layers.append(('act0', activation_layer))\n",
    "        layers.append(('view', View(-1)))\n",
    "        layers.append(('dense0', EqualizedLinear(in_channels2 * resolution * resolution, intermediate_channels,\n",
    "                                                 gain=gain, use_wscale=use_wscale)))\n",
    "        layers.append(('act1', activation_layer))\n",
    "        layers.append(('dense1', EqualizedLinear(intermediate_channels, output_features,\n",
    "                                                 gain=last_gain, use_wscale=use_wscale)))\n",
    "\n",
    "        super().__init__(OrderedDict(layers))\n",
    "\n",
    "\n",
    "class DiscriminatorBlock(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, gain, use_wscale, activation_layer, blur_kernel):\n",
    "        super().__init__(OrderedDict([\n",
    "            ('conv0', EqualizedConv2d(in_channels, in_channels, kernel_size=3, gain=gain, use_wscale=use_wscale)),\n",
    "            # out channels nf(res-1)\n",
    "            ('act0', activation_layer),\n",
    "            ('blur', BlurLayer(kernel=blur_kernel)),\n",
    "            ('conv1_down', EqualizedConv2d(in_channels, out_channels, kernel_size=3,\n",
    "                                           gain=gain, use_wscale=use_wscale, downscale=True)),\n",
    "            ('act1', activation_layer)]))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # discriminator = DiscriminatorTop()\n",
    "    print('Done.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "class GMapping(nn.Module):\n",
    "\n",
    "    def __init__(self, latent_size=100, dlatent_size=100, dlatent_broadcast=None,\n",
    "                 mapping_layers=8, mapping_fmaps=100, mapping_lrmul=0.01, mapping_nonlinearity='lrelu',\n",
    "                 use_wscale=True, normalize_latents=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Mapping network used in the StyleGAN paper.\n",
    "        :param latent_size: Latent vector(Z) dimensionality.\n",
    "        # :param label_size: Label dimensionality, 0 if no labels.\n",
    "        :param dlatent_size: Disentangled latent (W) dimensionality.\n",
    "        :param dlatent_broadcast: Output disentangled latent (W) as [minibatch, dlatent_size]\n",
    "                                  or [minibatch, dlatent_broadcast, dlatent_size].\n",
    "        :param mapping_layers: Number of mapping layers.\n",
    "        :param mapping_fmaps: Number of activations in the mapping layers.\n",
    "        :param mapping_lrmul: Learning rate multiplier for the mapping layers.\n",
    "        :param mapping_nonlinearity: Activation function: 'relu', 'lrelu'.\n",
    "        :param use_wscale: Enable equalized learning rate?\n",
    "        :param normalize_latents: Normalize latent vectors (Z) before feeding them to the mapping layers?\n",
    "        :param kwargs: Ignore unrecognized keyword args.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.latent_size = latent_size\n",
    "        self.mapping_fmaps = mapping_fmaps\n",
    "        self.dlatent_size = dlatent_size\n",
    "        self.dlatent_broadcast = dlatent_broadcast\n",
    "\n",
    "        # Activation function.\n",
    "        act, gain = {'relu': (torch.relu, np.sqrt(2)),\n",
    "                     'lrelu': (nn.LeakyReLU(negative_slope=0.2), np.sqrt(2))}[mapping_nonlinearity]\n",
    "\n",
    "        # Embed labels and concatenate them with latents.\n",
    "        # TODO\n",
    "\n",
    "        layers = []\n",
    "        # Normalize latents.\n",
    "        if normalize_latents:\n",
    "            layers.append(('pixel_norm', PixelNormLayer()))\n",
    "\n",
    "        # Mapping layers. (apply_bias?)\n",
    "        layers.append(('dense0', EqualizedLinear(self.latent_size, self.mapping_fmaps,\n",
    "                                                 gain=gain, lrmul=mapping_lrmul, use_wscale=use_wscale)))\n",
    "        layers.append(('dense0_act', act))\n",
    "        for layer_idx in range(1, mapping_layers):\n",
    "            fmaps_in = self.mapping_fmaps\n",
    "            fmaps_out = self.dlatent_size if layer_idx == mapping_layers - 1 else self.mapping_fmaps\n",
    "            layers.append(\n",
    "                ('dense{:d}'.format(layer_idx),\n",
    "                 EqualizedLinear(fmaps_in, fmaps_out, gain=gain, lrmul=mapping_lrmul, use_wscale=use_wscale)))\n",
    "            layers.append(('dense{:d}_act'.format(layer_idx), act))\n",
    "\n",
    "        # Output.\n",
    "        self.map = nn.Sequential(OrderedDict(layers))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First input: Latent vectors (Z) [mini_batch, latent_size].\n",
    "        x = self.map(x)\n",
    "\n",
    "        # Broadcast -> batch_size * dlatent_broadcast * dlatent_size\n",
    "        if self.dlatent_broadcast is not None:\n",
    "            x = x.unsqueeze(1).expand(-1, self.dlatent_broadcast, -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GSynthesis(nn.Module):\n",
    "\n",
    "    def __init__(self, dlatent_size=512, num_channels=3, resolution=1024,\n",
    "                 fmap_base=8192, fmap_decay=1.0, fmap_max=512,\n",
    "                 use_styles=True, const_input_layer=True, use_noise=True, nonlinearity='lrelu',\n",
    "                 use_wscale=True, use_pixel_norm=False, use_instance_norm=True, blur_filter=None,\n",
    "                 structure='linear', **kwargs):\n",
    "        \"\"\"\n",
    "        Synthesis network used in the StyleGAN paper.\n",
    "        :param dlatent_size: Disentangled latent (W) dimensionality.\n",
    "        :param num_channels: Number of output color channels.\n",
    "        :param resolution: Output resolution.\n",
    "        :param fmap_base: Overall multiplier for the number of feature maps.\n",
    "        :param fmap_decay: log2 feature map reduction when doubling the resolution.\n",
    "        :param fmap_max: Maximum number of feature maps in any layer.\n",
    "        :param use_styles: Enable style inputs?\n",
    "        :param const_input_layer: First layer is a learned constant?\n",
    "        :param use_noise: Enable noise inputs?\n",
    "        # :param randomize_noise: True = randomize noise inputs every time (non-deterministic),\n",
    "                                  False = read noise inputs from variables.\n",
    "        :param nonlinearity: Activation function: 'relu', 'lrelu'\n",
    "        :param use_wscale: Enable equalized learning rate?\n",
    "        :param use_pixel_norm: Enable pixel_wise feature vector normalization?\n",
    "        :param use_instance_norm: Enable instance normalization?\n",
    "        :param blur_filter: Low-pass filter to apply when resampling activations. None = no filtering.\n",
    "        :param structure: 'fixed' = no progressive growing, 'linear' = human-readable\n",
    "        :param kwargs: Ignore unrecognized keyword args.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # if blur_filter is None:\n",
    "        #     blur_filter = [1, 2, 1]\n",
    "\n",
    "        def nf(stage):\n",
    "            return min(int(fmap_base / (2.0 ** (stage * fmap_decay))), fmap_max)\n",
    "\n",
    "        self.structure = structure\n",
    "\n",
    "        resolution_log2 = int(np.log2(resolution))\n",
    "        assert resolution == 2 ** resolution_log2 and resolution >= 4\n",
    "        self.depth = resolution_log2 - 1\n",
    "\n",
    "        self.num_layers = resolution_log2 * 2 - 2\n",
    "        self.num_styles = self.num_layers if use_styles else 1\n",
    "\n",
    "        act, gain = {'relu': (torch.relu, np.sqrt(2)),\n",
    "                     'lrelu': (nn.LeakyReLU(negative_slope=0.2), np.sqrt(2))}[nonlinearity]\n",
    "\n",
    "        # Early layers.\n",
    "        self.init_block = InputBlock(nf(1), dlatent_size, const_input_layer, gain, use_wscale,\n",
    "                                     use_noise, use_pixel_norm, use_instance_norm, use_styles, act)\n",
    "        # create the ToRGB layers for various outputs\n",
    "        rgb_converters = [EqualizedConv2d(nf(1), num_channels, 1, gain=1, use_wscale=use_wscale)]\n",
    "\n",
    "        # Building blocks for remaining layers.\n",
    "        blocks = []\n",
    "        for res in range(3, resolution_log2 + 1):\n",
    "            last_channels = nf(res - 2)\n",
    "            channels = nf(res - 1)\n",
    "            # name = '{s}x{s}'.format(s=2 ** res)\n",
    "            blocks.append(GSynthesisBlock(last_channels, channels, blur_filter, dlatent_size, gain, use_wscale,\n",
    "                                          use_noise, use_pixel_norm, use_instance_norm, use_styles, act))\n",
    "            rgb_converters.append(EqualizedConv2d(channels, num_channels, 1, gain=1, use_wscale=use_wscale))\n",
    "\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "        self.to_rgb = nn.ModuleList(rgb_converters)\n",
    "\n",
    "        # register the temporary upsampler\n",
    "        self.temporaryUpsampler = lambda x: interpolate(x, scale_factor=2)\n",
    "\n",
    "    def forward(self, dlatents_in, depth=0, alpha=0., labels_in=None):\n",
    "        \"\"\"\n",
    "            forward pass of the Generator\n",
    "            :param dlatents_in: Input: Disentangled latents (W) [mini_batch, num_layers, dlatent_size].\n",
    "            :param labels_in:\n",
    "            :param depth: current depth from where output is required\n",
    "            :param alpha: value of alpha for fade-in effect\n",
    "            :return: y => output\n",
    "        \"\"\"\n",
    "\n",
    "        assert depth < self.depth, \"Requested output depth cannot be produced\"\n",
    "\n",
    "        if self.structure == 'fixed':\n",
    "            x = self.init_block(dlatents_in[:, 0:2])\n",
    "            for i, block in enumerate(self.blocks):\n",
    "                x = block(x, dlatents_in[:, 2 * (i + 1):2 * (i + 2)])\n",
    "            images_out = self.to_rgb[-1](x)\n",
    "        elif self.structure == 'linear':\n",
    "            x = self.init_block(dlatents_in[:, 0:2])\n",
    "\n",
    "            if depth > 0:\n",
    "                for i, block in enumerate(self.blocks[:depth - 1]):\n",
    "                    x = block(x, dlatents_in[:, 2 * (i + 1):2 * (i + 2)])\n",
    "\n",
    "                residual = self.to_rgb[depth - 1](self.temporaryUpsampler(x))\n",
    "                straight = self.to_rgb[depth](self.blocks[depth - 1](x, dlatents_in[:, 2 * depth:2 * (depth + 1)]))\n",
    "\n",
    "                images_out = (alpha * straight) + ((1 - alpha) * residual)\n",
    "            else:\n",
    "                images_out = self.to_rgb[0](x)\n",
    "        else:\n",
    "            raise KeyError(\"Unknown structure: \", self.structure)\n",
    "\n",
    "        return images_out\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, resolution, latent_size=100, dlatent_size=100,\n",
    "                 conditional=False, n_classes=0, truncation_psi=0.7,\n",
    "                 truncation_cutoff=8, dlatent_avg_beta=0.995,\n",
    "                 style_mixing_prob=0.9, **kwargs):\n",
    "        \"\"\"\n",
    "        # Style-based generator used in the StyleGAN paper.\n",
    "        # Composed of two sub-networks (G_mapping and G_synthesis).\n",
    "        :param resolution:\n",
    "        :param latent_size:\n",
    "        :param dlatent_size:\n",
    "        :param truncation_psi: Style strength multiplier for the truncation trick. None = disable.\n",
    "        :param truncation_cutoff: Number of layers for which to apply the truncation trick. None = disable.\n",
    "        :param dlatent_avg_beta: Decay for tracking the moving average of W during training. None = disable.\n",
    "        :param style_mixing_prob: Probability of mixing styles during training. None = disable.\n",
    "        :param kwargs: Arguments for sub-networks (G_mapping and G_synthesis).\n",
    "        \"\"\"\n",
    "\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        if conditional:\n",
    "            assert n_classes > 0, \"Conditional generation requires n_class > 0\"\n",
    "            self.class_embedding = nn.Embedding(n_classes, latent_size)\n",
    "            latent_size *= 2\n",
    "\n",
    "        self.conditional = conditional\n",
    "        self.style_mixing_prob = style_mixing_prob\n",
    "\n",
    "        # Setup components.\n",
    "        self.num_layers = (int(np.log2(resolution)) - 1) * 2\n",
    "        self.g_mapping = GMapping(latent_size, dlatent_size, dlatent_broadcast=self.num_layers, **kwargs)\n",
    "        self.g_synthesis = GSynthesis(resolution=resolution, **kwargs)\n",
    "\n",
    "        if truncation_psi > 0:\n",
    "            self.truncation = Truncation(avg_latent=torch.zeros(dlatent_size),\n",
    "                                         max_layer=truncation_cutoff,\n",
    "                                         threshold=truncation_psi,\n",
    "                                         beta=dlatent_avg_beta)\n",
    "        else:\n",
    "            self.truncation = None\n",
    "\n",
    "    def forward(self, latents_in, depth, alpha, labels_in=None):\n",
    "        \"\"\"\n",
    "        :param latents_in: First input: Latent vectors (Z) [mini_batch, latent_size].\n",
    "        :param depth: current depth from where output is required\n",
    "        :param alpha: value of alpha for fade-in effect\n",
    "        :param labels_in: Second input: Conditioning labels [mini_batch, label_size].\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        if not self.conditional:\n",
    "            if labels_in is not None:\n",
    "                warnings.warn(\n",
    "                    \"Generator is unconditional, labels_in will be ignored\")\n",
    "        else:\n",
    "            assert labels_in is not None, \"Conditional discriminatin requires labels\"\n",
    "            embedding = self.class_embedding(labels_in)\n",
    "            latents_in = torch.cat([latents_in, embedding], 1)\n",
    "\n",
    "        dlatents_in = self.g_mapping(latents_in)\n",
    "\n",
    "        if self.training:\n",
    "            # Update moving average of W(dlatent).\n",
    "            # TODO\n",
    "            if self.truncation is not None:\n",
    "                self.truncation.update(dlatents_in[0, 0].detach())\n",
    "\n",
    "            # Perform style mixing regularization.\n",
    "            if self.style_mixing_prob is not None and self.style_mixing_prob > 0:\n",
    "                latents2 = torch.randn(latents_in.shape).to(latents_in.device)\n",
    "                dlatents2 = self.g_mapping(latents2)\n",
    "                layer_idx = torch.from_numpy(np.arange(self.num_layers)[np.newaxis, :, np.newaxis]).to(\n",
    "                    latents_in.device)\n",
    "                cur_layers = 2 * (depth + 1)\n",
    "                mixing_cutoff = random.randint(1,\n",
    "                                               cur_layers) if random.random() < self.style_mixing_prob else cur_layers\n",
    "                dlatents_in = torch.where(layer_idx < mixing_cutoff, dlatents_in, dlatents2)\n",
    "\n",
    "            # Apply truncation trick.\n",
    "            if self.truncation is not None:\n",
    "                dlatents_in = self.truncation(dlatents_in)\n",
    "\n",
    "        fake_images = self.g_synthesis(dlatents_in, depth, alpha)\n",
    "\n",
    "        return fake_images"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "data": {
      "text/plain": "Sequential(\n  (pixel_norm): PixelNormLayer()\n  (dense0): EqualizedLinear()\n  (dense0_act): LeakyReLU(negative_slope=0.2)\n  (dense1): EqualizedLinear()\n  (dense1_act): LeakyReLU(negative_slope=0.2)\n  (dense2): EqualizedLinear()\n  (dense2_act): LeakyReLU(negative_slope=0.2)\n  (dense3): EqualizedLinear()\n  (dense3_act): LeakyReLU(negative_slope=0.2)\n  (dense4): EqualizedLinear()\n  (dense4_act): LeakyReLU(negative_slope=0.2)\n  (dense5): EqualizedLinear()\n  (dense5_act): LeakyReLU(negative_slope=0.2)\n  (dense6): EqualizedLinear()\n  (dense6_act): LeakyReLU(negative_slope=0.2)\n  (dense7): EqualizedLinear()\n  (dense7_act): LeakyReLU(negative_slope=0.2)\n)"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = Generator(128)\n",
    "G.g_mapping.map"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "data": {
      "text/plain": "80800"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(G.g_mapping.map)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3840x128 and 100x100)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_9144/2916077028.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mG\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mones\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m10\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m128\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m128\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mB:\\Anaconda_install_path\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1103\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_9144/1781503403.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, latents_in, depth, alpha, labels_in)\u001B[0m\n\u001B[0;32m     59\u001B[0m             \u001B[0mlatents_in\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mlatents_in\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0membedding\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     60\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 61\u001B[1;33m         \u001B[0mdlatents_in\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mg_mapping\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlatents_in\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     62\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     63\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtraining\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mB:\\Anaconda_install_path\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1103\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_9144/3639022286.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     56\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     57\u001B[0m         \u001B[1;31m# First input: Latent vectors (Z) [mini_batch, latent_size].\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 58\u001B[1;33m         \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmap\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     59\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     60\u001B[0m         \u001B[1;31m# Broadcast -> batch_size * dlatent_broadcast * dlatent_size\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mB:\\Anaconda_install_path\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1103\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mB:\\Anaconda_install_path\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    139\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    140\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 141\u001B[1;33m             \u001B[0minput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodule\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    142\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    143\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mB:\\Anaconda_install_path\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1103\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_9144/2311607111.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     93\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mbias\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     94\u001B[0m             \u001B[0mbias\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbias\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mb_mul\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 95\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlinear\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mweight\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mw_mul\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbias\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     96\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     97\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mB:\\Anaconda_install_path\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\functional.py\u001B[0m in \u001B[0;36mlinear\u001B[1;34m(input, weight, bias)\u001B[0m\n\u001B[0;32m   1846\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mhas_torch_function_variadic\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbias\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1847\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mhandle_torch_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlinear\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbias\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbias\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mbias\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1848\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_C\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_nn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlinear\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbias\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1849\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1850\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (3840x128 and 100x100)"
     ]
    }
   ],
   "source": [
    "G(torch.ones(10,3,128,128), 0, 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}