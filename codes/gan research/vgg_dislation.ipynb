{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Libs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "from torchvision.datasets import CelebA\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import math\n",
    "# main libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset\n",
    "import math\n",
    "import torchvision.transforms.functional as TF\n",
    "import cv2\n",
    "from keras.utils import image_dataset_from_directory\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "from torch.nn import init\n",
    "from torchvision.utils import make_grid\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from skimage import transform\n",
    "import skimage.io as io\n",
    "import numpy as np\n",
    "from torchvision.utils import save_image"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0\n",
      "torch.Size([3, 2])\n",
      "weight_hh_l0\n",
      "torch.Size([3, 3])\n",
      "bias_ih_l0\n",
      "torch.Size([3])\n",
      "bias_hh_l0\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "N, L, Hin, Hout = 10, 100, 2, 3\n",
    "\n",
    "m = nn.RNN(Hin,Hout, batch_first=True)\n",
    "x = torch.ones(N, L, Hin)\n",
    "\n",
    "out, h_out = m(x)\n",
    "\n",
    "h_out.shape\n",
    "\n",
    "for i in m.named_parameters():\n",
    "    print(i[0])\n",
    "    print(i[1].shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Helper Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28)):\n",
    "    '''\n",
    "    Function for visualizing images: Given a tensor of images, number of images, and\n",
    "    size per image, plots and prints the images in a uniform grid.\n",
    "    '''\n",
    "    image_unflat = image_tensor.detach().cpu().view(-1, *size)\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def ploter(image, image_hat):\n",
    "    \"\"\"\n",
    "    (H, W)\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    #plt.imshow(image_hat, cmap='gray', vmin=-1, vmax=1)\n",
    "    plt.imshow(image_hat)\n",
    "    plt.tight_layout()\n",
    "    plt.title(\"Reconstruct\")\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    #plt.imshow(image, cmap='gray', vmin=-1, vmax=1)\n",
    "    plt.imshow(image)\n",
    "    plt.tight_layout()\n",
    "    plt.title(\"Original\")\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Nets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in ./pretrain_models\\hub\\pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "os.environ['TORCH_HOME'] = './pretrain_models'\n",
    "VGG = torch.hub.load('pytorch/vision:v0.10.0', 'vgg11', pretrained=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        features = 64\n",
    "        self.features = features\n",
    "\n",
    "        self.init_size = 2\n",
    "        self.linear_layer = nn.Sequential(nn.Linear(latent_dim, (8*features) * (self.init_size**2)))\n",
    "\n",
    "        # VGG\n",
    "        #(64, 32, 32)\n",
    "        #(128, 16, 16)\n",
    "        #(256, 8, 8)\n",
    "        #(256, 8, 8)\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    # (512, 2, 2)\n",
    "\n",
    "                    (\"first_layer\", Generator._block(8*features, 4*features, False, 2)),\n",
    "                    # (256, 4, 4)\n",
    "\n",
    "                    (\"middle4\", Generator._block(4*features, 4*features, True, 2)), # TransferLearning\n",
    "                    # (256, 8, 8)\n",
    "\n",
    "                    (\"middle3\", Generator._block(4*features, 4*features, True, 1)), # TransferLearning\n",
    "                    # (256, 8, 8)\n",
    "\n",
    "                    (\"middle2\", Generator._block(4*features, 2*features, True, 2)), # TransferLearning\n",
    "                    # (128, 16, 16)\n",
    "\n",
    "                    (\"middle1\", Generator._block(2*features, 1*features, True, 2)), # TransferLearning\n",
    "                    # (64, 32, 32)\n",
    "\n",
    "                    (\"middle0\", Generator._block(1*features, 3, True, 1)),\n",
    "                    # (3, 32, 32)\n",
    "\n",
    "                    (\"last_layer\", nn.Conv2d(3, 1, kernel_size=3, stride=1, padding=1)),\n",
    "                    (\"tanh\", nn.Tanh())\n",
    "                    # (1, 32, 32)\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.linear_layer(z)\n",
    "        out = out.view(out.shape[0], 8*self.features, self.init_size, self.init_size)\n",
    "        image = self.conv_layers(out)\n",
    "        return image\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _block(in_channels, out_channels, activation, upsample):\n",
    "        if activation:\n",
    "            return nn.Sequential(\n",
    "                nn.BatchNorm2d(in_channels, 0.8),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Upsample(scale_factor=upsample),\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.BatchNorm2d(in_channels, 0.8),\n",
    "                nn.Upsample(scale_factor=upsample),\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "            )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# TEST\n",
    "\n",
    "N, z_dim = 50, 100\n",
    "C, H, W = 3, 32, 32\n",
    "\n",
    "z = torch.empty(N, z_dim)\n",
    "x = torch.ones(N, C, H, W).to(torch.float)\n",
    "G = Generator(100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def get_vgg_loss(Generator, z, VGG, image):\n",
    "\n",
    "    critic = nn.MSELoss()\n",
    "\n",
    "    linear_out = Generator.linear_layer(z).view(-1, 8*Generator.features, Generator.init_size, Generator.init_size)\n",
    "\n",
    "    # Layer 4 : (256, 8, 8)\n",
    "    loss_4 = critic(Generator.conv_layers[0:1+1](linear_out), VGG.features[0:9+1](image))\n",
    "    # Layer3 : (256, 8, 8)\n",
    "    loss_3 = critic(Generator.conv_layers[0:2+1](linear_out), VGG.features[0:7+1](image))\n",
    "    # Layer2  : (128, 16, 16)\n",
    "    loss_2 = critic(Generator.conv_layers[0:3+1](linear_out), VGG.features[0:4+1](image))\n",
    "    # Layer1 :  (64, 32, 32)\n",
    "    loss_1 = critic(Generator.conv_layers[0:4+1](linear_out), VGG.features[0:1+1](image))\n",
    "\n",
    "\n",
    "    return (1/4)*(loss_1 + loss_2 + loss_3 + loss_4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.6140, grad_fn=<MulBackward0>)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "\n",
    "get_vgg_loss(G, z, VGG, x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels, img_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            *discriminator_block(channels, 16, bn=False),\n",
    "            *discriminator_block(16, 32),\n",
    "            *discriminator_block(32, 64),\n",
    "            *discriminator_block(64, 128),\n",
    "        )\n",
    "\n",
    "        # self.ds_size = img_size // 2 ** 4 # for img_size = 96\n",
    "        self.ds_size = 2 # for img_size = 28\n",
    "        self.adverse_layer = nn.Sequential(nn.Linear(128 * self.ds_size ** 2, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, image):\n",
    "        out = self.conv_layers(image)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        validity = self.adverse_layer(out)\n",
    "        return validity"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 1, 32, 32])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "G = Generator(100)\n",
    "x = torch.ones(1, 1, 28, 28).to(torch.float)\n",
    "z = torch.FloatTensor(np.random.normal(0, 1, (x.shape[0], 100)))\n",
    "G(z).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 1])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "D = Discriminator(1, 28)\n",
    "x = torch.ones(1, 1, 28, 28).to(torch.float)\n",
    "D(x).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "root_save = \"./best_models\"\n",
    "root_ds = \"./MNIST\"\n",
    "\n",
    "batch_size = 128\n",
    "img_size = 32\n",
    "latent_dim = 100\n",
    "channels = 1\n",
    "num_epochs = 1000\n",
    "sample_interval = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "trans=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.ConvertImageDtype(torch.float),\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.Normalize((0.5,) , (0.5,))\n",
    "])\n",
    "\n",
    "train_data=datasets.MNIST(root=root_ds,\n",
    "                          train=True, transform=trans, download=True)\n",
    "test_data=datasets.MNIST(root=root_ds,\n",
    "                          train=False, transform=trans, download=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def get_index(class_num, dataset, mode:str):\n",
    "    if mode == 'train':\n",
    "        class_indx = torch.nonzero(\n",
    "                dataset.train_labels == class_num * torch.ones_like(dataset.train_labels)\n",
    "        )\n",
    "\n",
    "    if mode == 'test':\n",
    "        class_indx = torch.nonzero(\n",
    "                dataset.test_labels == class_num * torch.ones_like(dataset.test_labels)\n",
    "        )\n",
    "\n",
    "    class_indx = class_indx.squeeze(1)\n",
    "    return class_indx"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "B:\\Anaconda_install_path\\envs\\deeplearning\\lib\\site-packages\\torchvision\\datasets\\mnist.py:52: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n",
      "B:\\Anaconda_install_path\\envs\\deeplearning\\lib\\site-packages\\torchvision\\datasets\\mnist.py:57: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "train_sampler = SubsetRandomSampler(get_index(8, train_data, 'train'))\n",
    "test_sampler = SubsetRandomSampler(get_index(8, test_data, 'test'))\n",
    "\n",
    "train_loader=DataLoader(train_data, batch_size= batch_size, sampler=train_sampler)\n",
    "test_loader=DataLoader(test_data, batch_size= batch_size, sampler=test_sampler)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "generator = Generator(latent_dim=latent_dim).to(device)\n",
    "discriminator = Discriminator(channels=channels, img_size=img_size).to(device)\n",
    "VGG = VGG.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "adversarial_loss = nn.BCELoss()\n",
    "optimizer_G = optim.Adam(generator.parameters())\n",
    "optimizer_D = optim.Adam(discriminator.parameters())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0.0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "generator = generator.apply(weights_init_normal)\n",
    "discriminator = discriminator.apply(weights_init_normal)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_9584/1904144580.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     12\u001B[0m         \u001B[0mgen_imgs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgenerator\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mz\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m         \u001B[0mg_loss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0madversarial_loss\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdiscriminator\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgen_imgs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalid\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m+\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 14\u001B[1;33m                  \u001B[0mget_vgg_loss\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgenerator\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mz\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mVGG\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreal_images\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrepeat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     15\u001B[0m         \u001B[0mg_loss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     16\u001B[0m         \u001B[0moptimizer_G\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_9584/2134106403.py\u001B[0m in \u001B[0;36mget_vgg_loss\u001B[1;34m(Generator, z, VGG, image)\u001B[0m\n\u001B[0;32m     12\u001B[0m     \u001B[0mloss_2\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcritic\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mGenerator\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconv_layers\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlinear_out\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mVGG\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;36m4\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimage\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m     \u001B[1;31m# Layer1 :  (64, 32, 32)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 14\u001B[1;33m     \u001B[0mloss_1\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcritic\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mGenerator\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconv_layers\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;36m4\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlinear_out\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mVGG\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimage\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     15\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mB:\\Anaconda_install_path\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1103\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mB:\\Anaconda_install_path\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    139\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    140\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 141\u001B[1;33m             \u001B[0minput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodule\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    142\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    143\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mB:\\Anaconda_install_path\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1103\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mB:\\Anaconda_install_path\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    139\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    140\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 141\u001B[1;33m             \u001B[0minput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodule\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    142\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    143\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mB:\\Anaconda_install_path\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1103\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mB:\\Anaconda_install_path\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    444\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    445\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 446\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_conv_forward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbias\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    447\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    448\u001B[0m \u001B[1;32mclass\u001B[0m \u001B[0mConv3d\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_ConvNd\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mB:\\Anaconda_install_path\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001B[0m in \u001B[0;36m_conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    440\u001B[0m                             \u001B[0mweight\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbias\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstride\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    441\u001B[0m                             _pair(0), self.dilation, self.groups)\n\u001B[1;32m--> 442\u001B[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001B[0m\u001B[0;32m    443\u001B[0m                         self.padding, self.dilation, self.groups)\n\u001B[0;32m    444\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs):\n",
    "    total_G_loss = 0.0\n",
    "    total_D_loss = 0.0\n",
    "    for i, (real_images, _) in enumerate(train_loader):\n",
    "        valid = torch.FloatTensor(real_images.shape[0], 1).fill_(1.0).to(device)\n",
    "        fake = torch.FloatTensor(real_images.shape[0], 1).fill_(0.0).to(device)\n",
    "        real_images = real_images.to(device)\n",
    "\n",
    "        #  Train Generator\n",
    "        optimizer_G.zero_grad()\n",
    "        z = torch.FloatTensor(np.random.normal(0, 1, (real_images.shape[0], latent_dim))).to(device)\n",
    "        gen_imgs = generator(z)\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs), valid) +\\\n",
    "                 get_vgg_loss(generator, z, VGG, real_images.repeat(1,3,1,1))\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        total_G_loss += g_loss.cpu().detach().numpy()\n",
    "\n",
    "        #  Train Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        discriminator_opinion_real = discriminator(real_images)\n",
    "        discriminator_opinion_fake = discriminator(gen_imgs.detach())\n",
    "        real_loss = adversarial_loss(discriminator_opinion_real, valid)\n",
    "        fake_loss = adversarial_loss(discriminator_opinion_fake, fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        total_D_loss += d_loss.cpu().detach().numpy()\n",
    "\n",
    "    if epoch % sample_interval ==0:\n",
    "\n",
    "        # show a sample from generator\n",
    "        z = torch.FloatTensor(np.random.normal(0, 1, (real_images.shape[0], latent_dim))).to(device)\n",
    "        gen_imgs = generator(z)\n",
    "        show_tensor_images(gen_imgs)\n",
    "        show_tensor_images(real_images)\n",
    "\n",
    "\n",
    "\n",
    "        # show losses\n",
    "        print(\n",
    "                \"[Epoch {}/{}] \\t[D loss: {:.3f}] \\t[G loss: {:.3f}]\".format(\n",
    "                    epoch, num_epochs, total_D_loss, total_G_loss)\n",
    "            )\n",
    "\n",
    "        # save all\n",
    "        torch.save({\"epoch\": epoch,\n",
    "            \"state_dict_G\": generator.state_dict(),\n",
    "            \"state_dict_D\": discriminator.state_dict(),\n",
    "            \"optimizer_G\": optimizer_G.state_dict(),\n",
    "            \"optimizer_D\": optimizer_D.state_dict()\n",
    "           }, root_save + f\"/epoch{epoch}_disloss{total_D_loss}_genloss{total_G_loss}.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "if epoch % sample_interval == 0 and i % (len(dataloader)/5) == 0:\n",
    "    save_image(gen_imgs.data[0,0],\n",
    "               \"images/{}_{}.png\".format(str(epoch).zfill(len(str(num_epochs))),\n",
    "                                         str(i).zfill(len(str(len(dataloader))))),\n",
    "               normalize=True)\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### End"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x1adf6eff790>"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(\n",
    "    VGG.features[0:9+1](real_images.repeat(1,3,1,1))[0][2].detach().cpu()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss_4 = critic(Generator.conv_layers[0:1+1](linear_out), VGG.features[0:9+1](image))\n",
    "# Layer3 : (256, 8, 8)\n",
    "loss_3 = critic(Generator.conv_layers[0:2+1](linear_out), VGG.features[0:7+1](image))\n",
    "# Layer2  : (128, 16, 16)\n",
    "loss_2 = critic(Generator.conv_layers[0:3+1](linear_out), VGG.features[0:4+1](image))\n",
    "# Layer1 :  (64, 32, 32)\n",
    "loss_1 = critic(Generator.conv_layers[0:4+1](linear_out), VGG.features[0:1+1](image))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}