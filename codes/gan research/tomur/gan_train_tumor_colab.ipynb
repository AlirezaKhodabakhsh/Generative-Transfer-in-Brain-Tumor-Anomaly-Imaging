{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Libs"
   ],
   "metadata": {
    "collapsed": false,
    "id": "a4MEQDsIB_3E"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torchvision.datasets import CelebA\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import math\n",
    "# main libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset\n",
    "import math\n",
    "import torchvision.transforms.functional as TF\n",
    "import cv2\n",
    "from keras.utils import image_dataset_from_directory\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "from torch.nn import init\n",
    "from torchvision.utils import make_grid\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from skimage import transform\n",
    "import skimage.io as io\n",
    "import numpy as np\n",
    "from torchvision.utils import save_image\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import vgg16\n",
    "from pathlib import Path\n",
    "from torch.autograd import Variable"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "NB4mjCjQB_3L"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Anomaly_Dataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 root\n",
    "                 ):\n",
    "        super(Anomaly_Dataset, self).__init__()\n",
    "\n",
    "        self.data = Anomaly_Dataset.load_dataset(root)\n",
    "        self.image, self.label = Anomaly_Dataset.get_numpy(self.data)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        x, y =  self.image[item], self.label[item]\n",
    "\n",
    "        # RGB -> GRAY : (H, W)\n",
    "        x = x[:,:,0]\n",
    "\n",
    "        # (1, H, W)\n",
    "        x = Anomaly_Dataset.normalization(x)\n",
    "        #x = np.expand_dims(x, axis=0)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def load_dataset(path):\n",
    "        img_rows = 96\n",
    "        img_cols = 96\n",
    "        return image_dataset_from_directory(directory = path,\n",
    "                                               label_mode = 'int',\n",
    "                                               color_mode = 'rgb',\n",
    "                                               shuffle = False,\n",
    "                                               batch_size = None,\n",
    "                                               image_size = (img_rows, img_cols),\n",
    "                                               crop_to_aspect_ratio = True)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_numpy(PrefetchDataset):\n",
    "        \"\"\"\n",
    "        return:\n",
    "            (N, H, W, C) , (N,)\n",
    "        \"\"\"\n",
    "        images = []\n",
    "        labels = []\n",
    "        for (image, label) in PrefetchDataset:\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "        return np.array(images), np.array(labels)\n",
    "\n",
    "    @staticmethod\n",
    "    def rgb_2_gray(x):\n",
    "        \"\"\"\n",
    "        (H, W, C) --> (H, W)\n",
    "        \"\"\"\n",
    "        return cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    @staticmethod\n",
    "    def normalization(x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x : np.array : (H, W)\n",
    "\n",
    "        Return:\n",
    "            np.array : (H, W)\n",
    "        \"\"\"\n",
    "        x = x - x.min(keepdims=True)\n",
    "        x = x / x.max(keepdims=True)\n",
    "        x = x - 0.5\n",
    "        return  x / 0.5"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Aa6QStg-8Zys"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Helper Functions"
   ],
   "metadata": {
    "collapsed": false,
    "id": "x3trikDlB_3N"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def show_tensor_images(image_tensor, num_images=25, size=(3, 96, 96)):\n",
    "    '''\n",
    "    Function for visualizing images: Given a tensor of images, number of images, and\n",
    "    size per image, plots and prints the images in a uniform grid.\n",
    "    '''\n",
    "    image_unflat = image_tensor.detach().cpu().view(-1, *size)\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    plt.show()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "id_J8mSjB_3O"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "G5vVTWJIB_3O"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def ploter(image, image_hat):\n",
    "    \"\"\"\n",
    "    (H, W)\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    #plt.imshow(image_hat, cmap='gray', vmin=-1, vmax=1)\n",
    "    plt.imshow(image_hat)\n",
    "    plt.tight_layout()\n",
    "    plt.title(\"Reconstruct\")\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    #plt.imshow(image, cmap='gray', vmin=-1, vmax=1)\n",
    "    plt.imshow(image)\n",
    "    plt.tight_layout()\n",
    "    plt.title(\"Original\")\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "plW9T5H9B_3P"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generator Attention UNet"
   ],
   "metadata": {
    "collapsed": false,
    "id": "f8Q0DyE6B_3P"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class conv_block(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super(conv_block,self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=3,stride=1,padding=1,bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch_out, ch_out, kernel_size=3,stride=1,padding=1,bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        return x"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "gj4NM4uRB_3Q"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class up_conv(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super(up_conv,self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(ch_in,ch_out,kernel_size=3,stride=1,padding=1,bias=True),\n",
    "\t\t    nn.BatchNorm2d(ch_out),\n",
    "\t\t\tnn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.up(x)\n",
    "        return x"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Ma5GZ4_4B_3Q"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Attention_block(nn.Module):\n",
    "    def __init__(self,F_g,F_l,F_int):\n",
    "        super(Attention_block,self).__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "            )\n",
    "\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self,g,x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1+x1)\n",
    "        psi = self.psi(psi)\n",
    "\n",
    "        return x*psi"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "22r7A6-BB_3R"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class AttU_Net(nn.Module):\n",
    "    def __init__(self,img_ch=3,output_ch=3, features=2):\n",
    "        super(AttU_Net,self).__init__()\n",
    "\n",
    "        self.Maxpool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "\n",
    "        self.Conv1 = conv_block(ch_in=img_ch,ch_out=features*1)\n",
    "        self.Conv2 = conv_block(ch_in=features*1,ch_out=features*2)\n",
    "        self.Conv3 = conv_block(ch_in=features*2,ch_out=features*4)\n",
    "        self.Conv4 = conv_block(ch_in=features*4,ch_out=features*8)\n",
    "        self.Conv5 = conv_block(ch_in=features*8,ch_out=features*16)\n",
    "\n",
    "        self.Up5 = up_conv(ch_in=features*16,ch_out=features*8)\n",
    "        self.Att5 = Attention_block(F_g=features*8,F_l=features*8,F_int=features*4)\n",
    "        self.Up_conv5 = conv_block(ch_in=features*16, ch_out=features*8)\n",
    "\n",
    "        self.Up4 = up_conv(ch_in=features*8,ch_out=features*4)\n",
    "        self.Att4 = Attention_block(F_g=features*4,F_l=features*4,F_int=features*2)\n",
    "        self.Up_conv4 = conv_block(ch_in=features*8, ch_out=features*4)\n",
    "\n",
    "        self.Up3 = up_conv(ch_in=features*4,ch_out=features*2)\n",
    "        self.Att3 = Attention_block(F_g=features*2,F_l=features*2,F_int=features*1)\n",
    "        self.Up_conv3 = conv_block(ch_in=features*4, ch_out=features*2)\n",
    "\n",
    "        self.Up2 = up_conv(ch_in=features*2,ch_out=features*1)\n",
    "        self.Att2 = Attention_block(F_g=features*1,F_l=features*1,F_int=int(features/2))\n",
    "        self.Up_conv2 = conv_block(ch_in=features*2, ch_out=features*1)\n",
    "\n",
    "        self.Conv_1x1 = nn.Conv2d(features*1,output_ch,kernel_size=1,stride=1,padding=0)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        # encoding path\n",
    "        x1 = self.Conv1(x)\n",
    "\n",
    "        x2 = self.Maxpool(x1)\n",
    "        x2 = self.Conv2(x2)\n",
    "\n",
    "        x3 = self.Maxpool(x2)\n",
    "        x3 = self.Conv3(x3)\n",
    "\n",
    "        x4 = self.Maxpool(x3)\n",
    "        x4 = self.Conv4(x4)\n",
    "\n",
    "        x5 = self.Maxpool(x4)\n",
    "        x5 = self.Conv5(x5)\n",
    "\n",
    "        # decoding + concat path\n",
    "        d5 = self.Up5(x5)\n",
    "        x4 = self.Att5(g=d5,x=x4)\n",
    "        d5 = torch.cat((x4,d5),dim=1)\n",
    "        d5 = self.Up_conv5(d5)\n",
    "\n",
    "        d4 = self.Up4(d5)\n",
    "        x3 = self.Att4(g=d4,x=x3)\n",
    "        d4 = torch.cat((x3,d4),dim=1)\n",
    "        d4 = self.Up_conv4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4)\n",
    "        x2 = self.Att3(g=d3,x=x2)\n",
    "        d3 = torch.cat((x2,d3),dim=1)\n",
    "        d3 = self.Up_conv3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        x1 = self.Att2(g=d2,x=x1)\n",
    "        d2 = torch.cat((x1,d2),dim=1)\n",
    "        d2 = self.Up_conv2(d2)\n",
    "\n",
    "        d1 = self.Conv_1x1(d2)\n",
    "\n",
    "        return torch.tanh(d1)\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "z4ynwMx1B_3S"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "m = AttU_Net(3,3,8).to('cuda')\n",
    "z = torch.FloatTensor(np.random.normal(0, 1, (10, 3, 32, 32))).to('cuda')\n",
    "m(z).shape"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NVF7GIg_B_3S",
    "outputId": "87793d75-3300-4cbc-fd5d-365134033e17"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generator My Attention"
   ],
   "metadata": {
    "id": "2uD8_9ivPGU1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class conv_block_encoder(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super(conv_block_encoder,self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv2d(ch_in, ch_out, kernel_size=3,stride=1,padding=1,bias=True)),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.utils.spectral_norm(nn.Conv2d(ch_out, ch_out, kernel_size=3,stride=1,padding=1,bias=True)),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        return x"
   ],
   "metadata": {
    "id": "OZLBl7GuPJBw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class conv_block_decoder(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super(conv_block_decoder,self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv2d(ch_in, ch_out, kernel_size=3,stride=1,padding=1,bias=True)),\n",
    "            nn.utils.spectral_norm(nn.Conv2d(ch_out, ch_out, kernel_size=3,stride=1,padding=1,bias=True))\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        return x"
   ],
   "metadata": {
    "id": "8_AE1OxoPQFF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class up_conv(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super(up_conv,self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.utils.spectral_norm(nn.Conv2d(ch_in,ch_out,kernel_size=3,stride=1,padding=1,bias=True))\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.up(x)\n",
    "        return x"
   ],
   "metadata": {
    "id": "yziUHt3APR4N"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class Attention_block(nn.Module):\n",
    "    def __init__(self,F_g,F_l,F_int):\n",
    "        super(Attention_block,self).__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv2d(F_g, F_int, kernel_size=1,stride=1,padding=0,bias=False)),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "            )\n",
    "\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv2d(F_l, F_int, kernel_size=1,stride=1,padding=0,bias=False)),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv2d(F_int, 1, kernel_size=1,stride=1,padding=0,bias=False)),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self,g,x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1+x1)\n",
    "        psi = self.psi(psi)\n",
    "\n",
    "        return x*psi"
   ],
   "metadata": {
    "id": "Vs-jpxK1PUIo"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class MyAttU_Net(nn.Module):\n",
    "    def __init__(self,img_ch=3,output_ch=3, features=2):\n",
    "        super(MyAttU_Net,self).__init__()\n",
    "\n",
    "        self.Maxpool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "\n",
    "\n",
    "        # Encoder\n",
    "        self.Conv1 = conv_block_encoder(ch_in=img_ch,ch_out=features*1)\n",
    "        self.Conv2 = conv_block_encoder(ch_in=features*1,ch_out=features*2)\n",
    "        self.Conv3 = conv_block_encoder(ch_in=features*2,ch_out=features*4)\n",
    "        self.Conv4 = conv_block_encoder(ch_in=features*4,ch_out=features*8)\n",
    "        self.Conv5 = conv_block_encoder(ch_in=features*8,ch_out=features*16)\n",
    "\n",
    "\n",
    "        # Decoder\n",
    "        self.batch5 = nn.BatchNorm2d(features*16)\n",
    "        self.activation5 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.Up5 = up_conv(ch_in=features*16,ch_out=features*8)\n",
    "        self.Att5 = Attention_block(F_g=features*8,F_l=features*8,F_int=features*4)\n",
    "        self.Up_conv5 = conv_block_decoder(ch_in=features*16, ch_out=features*8)\n",
    "\n",
    "\n",
    "        self.batch4 = nn.BatchNorm2d(features*8)\n",
    "        self.activation4 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.Up4 = up_conv(ch_in=features*8,ch_out=features*4)\n",
    "        self.Att4 = Attention_block(F_g=features*4,F_l=features*4,F_int=features*2)\n",
    "        self.Up_conv4 = conv_block_decoder(ch_in=features*8, ch_out=features*4)\n",
    "\n",
    "\n",
    "        self.batch3 = nn.BatchNorm2d(features*4)\n",
    "        self.activation3 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.Up3 = up_conv(ch_in=features*4,ch_out=features*2)\n",
    "        self.Att3 = Attention_block(F_g=features*2,F_l=features*2,F_int=features*1)\n",
    "        self.Up_conv3 = conv_block_decoder(ch_in=features*4, ch_out=features*2)\n",
    "\n",
    "\n",
    "\n",
    "        self.batch2 = nn.BatchNorm2d(features*2)\n",
    "        self.activation2 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.Up2 = up_conv(ch_in=features*2,ch_out=features*1)\n",
    "        self.Att2 = Attention_block(F_g=features*1,F_l=features*1,F_int=int(features/2))\n",
    "        self.Up_conv2 = conv_block_decoder(ch_in=features*2, ch_out=features*1)\n",
    "\n",
    "        self.Conv_1x1 = nn.utils.spectral_norm(nn.Conv2d(features*1,output_ch,kernel_size=1,stride=1,padding=0))\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        # encoding path\n",
    "        x1 = self.Conv1(x)\n",
    "\n",
    "        x2 = self.Maxpool(x1)\n",
    "        x2 = self.Conv2(x2)\n",
    "\n",
    "        x3 = self.Maxpool(x2)\n",
    "        x3 = self.Conv3(x3)\n",
    "\n",
    "        x4 = self.Maxpool(x3)\n",
    "        x4 = self.Conv4(x4)\n",
    "\n",
    "        x5 = self.Maxpool(x4)\n",
    "        x5 = self.Conv5(x5)\n",
    "\n",
    "        # decoding + concat path\n",
    "        d5 = self.batch5(x5)\n",
    "        d5 = self.activation5(d5)\n",
    "        d5 = self.Up5(d5)\n",
    "        x4 = self.Att5(g=d5,x=x4)\n",
    "        d5 = torch.cat((x4,d5),dim=1)\n",
    "        d5 = self.Up_conv5(d5)\n",
    "\n",
    "        d4 = self.batch4(d5)\n",
    "        d4 = self.activation4(d4)\n",
    "        d4 = self.Up4(d4)\n",
    "        x3 = self.Att4(g=d4,x=x3)\n",
    "        d4 = torch.cat((x3,d4),dim=1)\n",
    "        d4 = self.Up_conv4(d4)\n",
    "\n",
    "        d3 = self.batch3(d4)\n",
    "        d3 = self.activation3(d3)\n",
    "        d3 = self.Up3(d3)\n",
    "        x2 = self.Att3(g=d3,x=x2)\n",
    "        d3 = torch.cat((x2,d3),dim=1)\n",
    "        d3 = self.Up_conv3(d3)\n",
    "\n",
    "        d2 = self.batch2(d3)\n",
    "        d2 = self.activation2(d2)\n",
    "        d2 = self.Up2(d2)\n",
    "        x1 = self.Att2(g=d2,x=x1)\n",
    "        d2 = torch.cat((x1,d2),dim=1)\n",
    "        d2 = self.Up_conv2(d2)\n",
    "\n",
    "        d1 = self.Conv_1x1(d2)\n",
    "\n",
    "        return torch.tanh(d1)\n"
   ],
   "metadata": {
    "id": "GFj-yt5UPX2X"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Simple Generator"
   ],
   "metadata": {
    "collapsed": false,
    "id": "z7uK0FG2pzF9"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, img_size, latent_dim, channels):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.init_size = img_size // 4\n",
    "        self.linear_layer = nn.Sequential(nn.Linear(latent_dim, 128 * self.init_size ** 2))\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.linear_layer(z)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        image = self.conv_layers(out)\n",
    "        return image"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "spYfZnR3B_3T"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Simple Discriminator"
   ],
   "metadata": {
    "id": "-Ig47-2PP2jn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels, img_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            *discriminator_block(channels, 16, bn=False),\n",
    "            *discriminator_block(16, 32),\n",
    "            *discriminator_block(32, 64),\n",
    "            *discriminator_block(64, 128),\n",
    "        )\n",
    "\n",
    "        # self.ds_size = img_size // 2 ** 4 # for img_size = 96\n",
    "        self.ds_size = 2 # for img_size = 28\n",
    "        self.adverse_layer = nn.Sequential(nn.Linear(128 * self.ds_size ** 2, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, image):\n",
    "        out = self.conv_layers(image)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        validity = self.adverse_layer(out)\n",
    "        return validity"
   ],
   "metadata": {
    "id": "VzMYs46vP5Al"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Discriminator Salehi"
   ],
   "metadata": {
    "collapsed": false,
    "id": "ggJ--N-TpzF-"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    '''\n",
    "    VGG model\n",
    "    '''\n",
    "\n",
    "    def __init__(self, features):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "\n",
    "        # placeholder for the gradients\n",
    "        self.gradients = None\n",
    "        self.activation = None\n",
    "\n",
    "    # hook for the gradients of the activations\n",
    "    def activations_hook(self, grad):\n",
    "        self.gradients = grad\n",
    "\n",
    "    def forward(self, x, target_layer=11):\n",
    "        result = []\n",
    "        for i in range(len(nn.ModuleList(self.features))):\n",
    "            x = self.features[i](x)\n",
    "            if i == target_layer:\n",
    "                self.activation = x\n",
    "                h = x.register_hook(self.activations_hook)\n",
    "            if i == 2 or i == 5 or i == 8 or i == 11 or i == 14 or i == 17 or i == 20 or i == 23 or i == 26 or i == 29 or i == 32 or i == 35 or i == 38:\n",
    "                result.append(x)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_activations_gradient(self):\n",
    "        return self.gradients\n",
    "\n",
    "    def get_activations(self, x):\n",
    "        return self.activation"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "yFNlTluTpzF-"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Vgg16(torch.nn.Module):\n",
    "    def __init__(self, pretrain):\n",
    "        super(Vgg16, self).__init__()\n",
    "        #features = list(vgg16('vgg16-397923af.pth').features)\n",
    "\n",
    "        features = list(vgg16(torchvision.models.VGG16_Weights).features)\n",
    "\n",
    "        if not pretrain:\n",
    "            for ind, f in enumerate(features):\n",
    "                # nn.init.xavier_normal_(f)\n",
    "                if type(f) is torch.nn.modules.conv.Conv2d:\n",
    "                    torch.nn.init.xavier_uniform(f.weight)\n",
    "                    print(\"Initialized\", ind, f)\n",
    "                else:\n",
    "                    print(\"Bypassed\", ind, f)\n",
    "            # print(\"Pre-trained Network loaded\")\n",
    "        self.features = nn.ModuleList(features).eval()\n",
    "        self.output = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = []\n",
    "        for i in range(31):\n",
    "            x = self.features[i](x)\n",
    "            if i == 1 or i == 4 or i == 6 or i == 9 or i == 11 or i == 13 or i == 16 or i == 18 or i == 20 or i == 23 or i == 25 or i == 27 or i == 30:\n",
    "                output.append(x)\n",
    "        return output"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "IKmnO_xApzF_"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_layers(cfg, use_bias, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    outputs = []\n",
    "    for i in range(len(cfg)):\n",
    "        if cfg[i] == 'O':\n",
    "            outputs.append(nn.Sequential(*layers))\n",
    "        elif cfg[i] == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, cfg[i], kernel_size=3, padding=1, bias=use_bias)\n",
    "            torch.nn.init.xavier_uniform_(conv2d.weight)\n",
    "            if batch_norm and cfg[i + 1] != 'M':\n",
    "                layers += [conv2d, nn.BatchNorm2d(cfg[i]), nn.LeakyReLU(0.2, inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.LeakyReLU(0.2, inplace=True)]\n",
    "            in_channels = cfg[i]\n",
    "    return nn.Sequential(*layers)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "d3WZQcXIpzF_"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_arch(idx, cfg, use_bias, batch_norm=False):\n",
    "    return VGG(make_layers(cfg[idx], use_bias, batch_norm=batch_norm))"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "c9stsU2CpzF_"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_networks(config, load_checkpoint=False):\n",
    "    equal_network_size = config['equal_network_size']\n",
    "    pretrain = config['pretrain']\n",
    "    experiment_name = config['experiment_name']\n",
    "    dataset_name = config['dataset_name']\n",
    "    normal_class = config['normal_class']\n",
    "    use_bias = config['use_bias']\n",
    "    cfg = {\n",
    "        'A': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "        'B': [16, 16, 'M', 16, 128, 'M', 16, 16, 256, 'M', 16, 16, 512, 'M', 16, 16, 512, 'M'],\n",
    "    }\n",
    "\n",
    "    if equal_network_size:\n",
    "        config_type = 'A'\n",
    "    else:\n",
    "        config_type = 'B'\n",
    "\n",
    "    vgg = Vgg16(pretrain).cuda()\n",
    "    model = make_arch(config_type, cfg, use_bias, True).cuda()\n",
    "\n",
    "    for j, item in enumerate(nn.ModuleList(model.features)):\n",
    "        print('layer : {} {}'.format(j, item))\n",
    "\n",
    "    if load_checkpoint:\n",
    "        last_checkpoint = config['last_checkpoint']\n",
    "        checkpoint_path = \"./outputs/{}/{}/checkpoints/\".format(experiment_name, dataset_name)\n",
    "        model.load_state_dict(\n",
    "            torch.load('{}Cloner_{}_epoch_{}.pth'.format(checkpoint_path, normal_class, last_checkpoint)))\n",
    "        if not pretrain:\n",
    "            vgg.load_state_dict(\n",
    "                torch.load('{}Source_{}_random_vgg.pth'.format(checkpoint_path, normal_class)))\n",
    "    elif not pretrain:\n",
    "        checkpoint_path = \"./outputs/{}/{}/checkpoints/\".format(experiment_name, dataset_name)\n",
    "        Path(checkpoint_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        torch.save(vgg.state_dict(), '{}Source_{}_random_vgg.pth'.format(checkpoint_path, normal_class))\n",
    "        print(\"Source Checkpoint saved!\")\n",
    "\n",
    "    return vgg, model"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "0KDhy_ZhpzGA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        _, self.vgg = get_networks(config)\n",
    "        self.adverse_layer = nn.Linear(512*3*3 , 1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.adverse_layer(\n",
    "            self.vgg(x)[-1].flatten(1)\n",
    "        )"
   ],
   "metadata": {
    "id": "CR7pLRvrAY-J"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DirectionOnlyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DirectionOnlyLoss, self).__init__()\n",
    "        self.similarity_loss = torch.nn.CosineSimilarity()\n",
    "\n",
    "    def forward(self, output_pred, output_real):\n",
    "        y_pred_0, y_pred_1, y_pred_2, y_pred_3 = output_pred[3], output_pred[6], output_pred[9], output_pred[12]\n",
    "        y_0, y_1, y_2, y_3 = output_real[3], output_real[6], output_real[9], output_real[12]\n",
    "\n",
    "        loss_0 = torch.mean(1 - self.similarity_loss(y_pred_0.view(y_pred_0.shape[0], -1), y_0.view(y_0.shape[0], -1)))\n",
    "        loss_1 = torch.mean(1 - self.similarity_loss(y_pred_1.view(y_pred_1.shape[0], -1), y_1.view(y_1.shape[0], -1)))\n",
    "        loss_2 = torch.mean(1 - self.similarity_loss(y_pred_2.view(y_pred_2.shape[0], -1), y_2.view(y_2.shape[0], -1)))\n",
    "        loss_3 = torch.mean(1 - self.similarity_loss(y_pred_3.view(y_pred_3.shape[0], -1), y_3.view(y_3.shape[0], -1)))\n",
    "\n",
    "        total_loss = loss_0 + loss_1 + loss_2 + loss_3\n",
    "\n",
    "        return"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "byI9SmGwpzGA"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MseDirectionLoss(nn.Module):\n",
    "    def __init__(self, lamda):\n",
    "        super(MseDirectionLoss, self).__init__()\n",
    "        self.lamda = lamda\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.similarity_loss = torch.nn.CosineSimilarity()\n",
    "\n",
    "    def forward(self, output_pred, output_real):\n",
    "        y_pred_0, y_pred_1, y_pred_2, y_pred_3 = output_pred[3], output_pred[6], output_pred[9], output_pred[12]\n",
    "        y_0, y_1, y_2, y_3 = output_real[3], output_real[6], output_real[9], output_real[12]\n",
    "\n",
    "        # different terms of loss\n",
    "        abs_loss_0 = self.criterion(y_pred_0, y_0)\n",
    "        loss_0 = torch.mean(1 - self.similarity_loss(y_pred_0.view(y_pred_0.shape[0], -1), y_0.view(y_0.shape[0], -1)))\n",
    "        abs_loss_1 = self.criterion(y_pred_1, y_1)\n",
    "        loss_1 = torch.mean(1 - self.similarity_loss(y_pred_1.view(y_pred_1.shape[0], -1), y_1.view(y_1.shape[0], -1)))\n",
    "        abs_loss_2 = self.criterion(y_pred_2, y_2)\n",
    "        loss_2 = torch.mean(1 - self.similarity_loss(y_pred_2.view(y_pred_2.shape[0], -1), y_2.view(y_2.shape[0], -1)))\n",
    "        abs_loss_3 = self.criterion(y_pred_3, y_3)\n",
    "        loss_3 = torch.mean(1 - self.similarity_loss(y_pred_3.view(y_pred_3.shape[0], -1), y_3.view(y_3.shape[0], -1)))\n",
    "\n",
    "        total_loss = loss_0 + loss_1 + loss_2 + loss_3 + self.lamda * (\n",
    "                abs_loss_0 + abs_loss_1 + abs_loss_2 + abs_loss_3)\n",
    "\n",
    "        return total_loss"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "vq5xUkM3pzGB"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameters"
   ],
   "metadata": {
    "collapsed": false,
    "id": "_ywxVYUjB_3V"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "root_save = \"/content/drive/MyDrive/MRI Arman/best_models\"\n",
    "root_ds = \"/content/drive/MyDrive/MRI Arman/dataset/kaggle1/no\"\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "batch_size = 46 #46\n",
    "img_size = 96\n",
    "latent_dim = 100\n",
    "channels = 3\n",
    "num_epochs = 2000\n",
    "sample_interval = 50\n",
    "lr = 2e-3\n",
    "n_critic = 5\n",
    "lambda_gp =10\n",
    "\n",
    "num_sub = 92\n",
    "init_features = 32"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "dImR2w4YB_3V"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "id": "by7w19vYB_3V"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = Anomaly_Dataset(root_ds)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset, batch_size=dataset.__len__(), shuffle=True)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oy2KBMQT8Zy8",
    "outputId": "d34c4154-59bc-4894-c694-acc4740304cf"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loss WGAN"
   ],
   "metadata": {
    "collapsed": false,
    "id": "3zlbPPltB_3W"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "Tensor = torch.cuda.FloatTensor # Cuda\n",
    "import torch.autograd as autograd\n",
    "\n",
    "def compute_gradient_penalty(discriminator, real_samples, fake_samples):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "    alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    d_interpolates = discriminator(interpolates)\n",
    "    fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "    # Get gradient w.r.t. interpolates\n",
    "    gradients = autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "u1CLQb7aEdRS"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### All together"
   ],
   "metadata": {
    "collapsed": false,
    "id": "v3mFaCTVpzGD"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config = {'equal_network_size':False,\n",
    "'pretrain':True,\n",
    "'experiment_name':'local_equal_net',\n",
    "'dataset_name':'mnist',\n",
    "'normal_class':1,\n",
    "'use_bias':False,\n",
    "'equal_network_size':False}\n",
    "\n",
    "# Criteria And Optimizers\n",
    "direction_loss_only = False\n",
    "lamda = 0.01\n",
    "\n",
    "if direction_loss_only:\n",
    "    criterion_disc_layers = DirectionOnlyLoss()\n",
    "else:\n",
    "    criterion_disc_layers = MseDirectionLoss(lamda)\n",
    "\n",
    "if direction_loss_only:\n",
    "    criterion_gen_layers = DirectionOnlyLoss()\n",
    "else:\n",
    "    criterion_gen_layers = MseDirectionLoss(lamda)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "llqVvRoRpzGD"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "VGG_11 = torch.hub.load('pytorch/vision:v0.10.0', 'vgg11', pretrained=True)\n",
    "VGG_11 = VGG_11.to(device)\n",
    "\n",
    "\n",
    "discriminator = Discriminator(config).cuda()\n",
    "vgg, _ = get_networks(config)\n",
    "\n",
    "generator = MyAttU_Net(channels,channels,init_features).to(device)\n",
    "#generator = Generator(img_size=img_size, latent_dim=latent_dim, channels=channels).cuda()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "jIPmH067B_3X",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f008c1e7-6c52-42c8-9d42-c542c63bae99"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "count_parameters(generator)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bt-yaW4EvSQP",
    "outputId": "f205bbd1-6a22-416b-d0b7-22ac359e72e0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "count_parameters(discriminator)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EVz9WeV1vWJP",
    "outputId": "2826bb38-94b1-4020-dcaf-729733426ef1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#adversarial_loss = nn.BCELoss()\n",
    "feature_loss = nn.MSELoss()\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr,  betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr,  betas=(0.5, 0.999))"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "OA1uX3RAB_3X"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0.0)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "FOkqnCrOB_3X"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generator = generator.apply(weights_init_normal)\n",
    "discriminator = discriminator.apply(weights_init_normal)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "VNzaUe66B_3X"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train"
   ],
   "metadata": {
    "collapsed": false,
    "id": "lhziaqqmB_3X"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# bia va b discri ham hal bede \n",
    "\n",
    "for epoch in range(1, num_epochs):\n",
    "    total_G_loss = 0.0\n",
    "    total_D_loss = 0.0\n",
    "    for i, (real_images, _) in enumerate(dataloader):\n",
    "        real_images = real_images.unsqueeze(1)\n",
    "        # Real and Fake images\n",
    "        if real_images.shape[1] == 1:\n",
    "            real_images = real_images.repeat(1, 3, 1, 1)\n",
    "        real_images = Variable(real_images).to(device)\n",
    "\n",
    "        # Simple Generator\n",
    "        #z = torch.FloatTensor(np.random.normal(0, 1, (real_images.shape[0], latent_dim))).to(device)\n",
    "        # Attention Generator\n",
    "        z = torch.FloatTensor(np.random.normal(0, 1, (real_images.shape[0], channels, img_size, img_size))).to(device)\n",
    "\n",
    "        gen_imgs = generator(z)\n",
    "\n",
    "        ####################  Train Discriminator ####################\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        ##########  A. Adverserial Loss\n",
    "        real_validity = discriminator(real_images)\n",
    "        fake_validity = discriminator(gen_imgs.detach())\n",
    "        gradient_penalty = compute_gradient_penalty(discriminator, real_images, gen_imgs.detach())\n",
    "        d_loss_adv = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n",
    "\n",
    "        ##########  B. Layers Loss\n",
    "        d_loss_layers_real = criterion_disc_layers(discriminator.vgg(real_images), vgg(real_images))\n",
    "        #d_loss_layers_fake = criterion_disc_layers(discriminator.forward(gen_imgs), vgg(gen_imgs))\n",
    "        #d_loss_layers = 0.5*(d_loss_layers_real + d_loss_layers_fake)\n",
    "\n",
    "        ##########  C. All Losses\n",
    "        #d_loss = 0.5*(d_loss_adv + d_loss_layers)\n",
    "        d_loss = d_loss_adv + 0.005*d_loss_layers_real\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        total_D_loss += d_loss.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "        ####################  Train Generator ####################\n",
    "        if i % n_critic == 0:\n",
    "\n",
    "            # Simple Generator\n",
    "            #z = torch.FloatTensor(np.random.normal(0, 1, (real_images.shape[0], latent_dim))).to(device)\n",
    "            # Attention Generator\n",
    "            z = torch.FloatTensor(np.random.normal(0, 1, (real_images.shape[0], channels, img_size, img_size))).to(device)\n",
    "        \n",
    "            gen_imgs = generator(z)\n",
    "\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "            ##########  A. Adverserial Loss\n",
    "            fake_validity = discriminator(gen_imgs)\n",
    "            g_loss_adv = -torch.mean(fake_validity)\n",
    "\n",
    "\n",
    "            ##########  B. Feature Loss\n",
    "\n",
    "            feature1_loss = feature_loss(\n",
    "                VGG_11.features[0:2](real_images),\n",
    "                VGG_11.features[0:2](gen_imgs)\n",
    "            )\n",
    "            feature2_loss = feature_loss(\n",
    "                VGG_11.features[0:5](real_images),\n",
    "                VGG_11.features[0:5](gen_imgs)\n",
    "            )\n",
    "            feature3_loss = feature_loss(\n",
    "                VGG_11.features[0:8](real_images),\n",
    "                VGG_11.features[0:8](gen_imgs)\n",
    "            )\n",
    "            g_loss_feature = (feature1_loss + feature2_loss + feature3_loss)\n",
    "\n",
    "            #g_loss_feature = criterion_gen_layers(vgg(gen_imgs), vgg(real_images))\n",
    "\n",
    "            ##########  C. Pixel Loss\n",
    "            g_loss_pixel = feature_loss(real_images, gen_imgs)\n",
    "\n",
    "\n",
    "            ##########  C. All together\n",
    "            g_loss = g_loss_adv + 0.005*g_loss_feature + 2e-4*g_loss_pixel\n",
    "            g_loss.backward(retain_graph=True)\n",
    "            optimizer_G.step()\n",
    "            total_G_loss += g_loss.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "    if epoch % sample_interval ==0:\n",
    "      for _, (real_images, _) in enumerate(test_loader):\n",
    "\n",
    "            real_images = real_images.unsqueeze(1)\n",
    "            # Real and Fake images\n",
    "            if real_images.shape[1] == 1:\n",
    "                real_images = real_images.repeat(1, 3, 1, 1)\n",
    "            real_images = Variable(real_images).to(device)\n",
    "\n",
    "            num = math.floor(\n",
    "                  np.random.uniform(0,test_loader.dataset.__len__()-30)\n",
    "              )\n",
    "            real_images = real_images[num:num+25]\n",
    "\n",
    "            # Attention\n",
    "            z = torch.FloatTensor(np.random.normal(0, 1, (real_images.shape[0], channels, img_size, img_size))).to(device)\n",
    "\n",
    "            gen_imgs = generator(z)\n",
    "            show_tensor_images(gen_imgs)\n",
    "            show_tensor_images(real_images)\n",
    "\n",
    "      # show losses\n",
    "      print(\n",
    "              \"[Epoch {}/{}] \\t[D loss: {:.3f}] \\t[G loss: {:.3f}]\".format(\n",
    "                  epoch, num_epochs, total_D_loss, total_G_loss)\n",
    "          )\n",
    "      # save all\n",
    "      torch.save({\"epoch\": epoch,\n",
    "          \"state_dict_G\": generator.state_dict(),\n",
    "          \"state_dict_D\": discriminator.state_dict(),\n",
    "          \"optimizer_G\": optimizer_G.state_dict(),\n",
    "          \"optimizer_D\": optimizer_D.state_dict()\n",
    "          }, root_save + f\"/epoch_{epoch}.pt\")"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "6EVo6xT7B_3Y"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for _, (real_images, _) in enumerate(test_loader):\n",
    "\n",
    "      real_images = real_images.unsqueeze(1)\n",
    "      # Real and Fake images\n",
    "      if real_images.shape[1] == 1:\n",
    "          real_images = real_images.repeat(1, 3, 1, 1)\n",
    "      real_images = Variable(real_images).to(device)\n",
    "\n",
    "      num = math.floor(\n",
    "            np.random.uniform(0,test_loader.dataset.__len__()-25)\n",
    "        )\n",
    "      real_images = real_images[num:num+25]\n",
    "\n",
    "      # Attention\n",
    "      z = torch.FloatTensor(np.random.normal(0, 1, (real_images.shape[0], channels, img_size, img_size))).to(device)\n",
    "\n",
    "      gen_imgs = generator(z)\n",
    "      show_tensor_images(gen_imgs)\n",
    "      show_tensor_images(real_images)"
   ],
   "metadata": {
    "id": "Bka0HWZ0ApnL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "if epoch % sample_interval == 0 and i % (len(dataloader)/5) == 0:\n",
    "    save_image(gen_imgs.data[0,0],\n",
    "               \"images/{}_{}.png\".format(str(epoch).zfill(len(str(num_epochs))),\n",
    "                                         str(i).zfill(len(str(len(dataloader))))),\n",
    "               normalize=True)\n",
    "\"\"\""
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "MyoS8TaRB_3Y"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### End"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "2hMi5zqYB_3Y"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "5n38_wwC_6FA"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}