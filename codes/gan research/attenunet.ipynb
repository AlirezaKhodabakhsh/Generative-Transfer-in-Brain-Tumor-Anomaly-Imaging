{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Libs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from torchvision.datasets import CelebA\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import math\n",
    "# main libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset\n",
    "import math\n",
    "import torchvision.transforms.functional as TF\n",
    "import cv2\n",
    "from keras.utils import image_dataset_from_directory\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "from torch.nn import init\n",
    "from torchvision.utils import make_grid\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from skimage import transform\n",
    "import skimage.io as io\n",
    "import numpy as np\n",
    "from torchvision.utils import save_image"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Helper Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def show_tensor_images(image_tensor, num_images=25, size=(1, 32, 32)):\n",
    "    '''\n",
    "    Function for visualizing images: Given a tensor of images, number of images, and\n",
    "    size per image, plots and prints the images in a uniform grid.\n",
    "    '''\n",
    "    image_unflat = image_tensor.detach().cpu().view(-1, *size)\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def ploter(image, image_hat):\n",
    "    \"\"\"\n",
    "    (H, W)\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    #plt.imshow(image_hat, cmap='gray', vmin=-1, vmax=1)\n",
    "    plt.imshow(image_hat)\n",
    "    plt.tight_layout()\n",
    "    plt.title(\"Reconstruct\")\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    #plt.imshow(image, cmap='gray', vmin=-1, vmax=1)\n",
    "    plt.imshow(image)\n",
    "    plt.tight_layout()\n",
    "    plt.title(\"Original\")\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "class conv_block_encoder(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super(conv_block_encoder,self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv2d(ch_in, ch_out, kernel_size=3,stride=1,padding=1,bias=True)),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.utils.spectral_norm(nn.Conv2d(ch_out, ch_out, kernel_size=3,stride=1,padding=1,bias=True)),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "class conv_block_decoder(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super(conv_block_decoder,self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv2d(ch_in, ch_out, kernel_size=3,stride=1,padding=1,bias=True)),\n",
    "            nn.utils.spectral_norm(nn.Conv2d(ch_out, ch_out, kernel_size=3,stride=1,padding=1,bias=True))\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "class up_conv(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super(up_conv,self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.utils.spectral_norm(nn.Conv2d(ch_in,ch_out,kernel_size=3,stride=1,padding=1,bias=True))\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.up(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "class Attention_block(nn.Module):\n",
    "    def __init__(self,F_g,F_l,F_int):\n",
    "        super(Attention_block,self).__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv2d(F_g, F_int, kernel_size=1,stride=1,padding=0,bias=False)),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "            )\n",
    "\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv2d(F_l, F_int, kernel_size=1,stride=1,padding=0,bias=False)),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv2d(F_int, 1, kernel_size=1,stride=1,padding=0,bias=False)),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self,g,x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1+x1)\n",
    "        psi = self.psi(psi)\n",
    "\n",
    "        return x*psi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "class AttU_Net(nn.Module):\n",
    "    def __init__(self,img_ch=1,output_ch=1, features=2):\n",
    "        super(AttU_Net,self).__init__()\n",
    "\n",
    "        self.Maxpool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "\n",
    "\n",
    "        # Encoder\n",
    "        self.Conv1 = conv_block_encoder(ch_in=img_ch,ch_out=features*1)\n",
    "        self.Conv2 = conv_block_encoder(ch_in=features*1,ch_out=features*2)\n",
    "        self.Conv3 = conv_block_encoder(ch_in=features*2,ch_out=features*4)\n",
    "        self.Conv4 = conv_block_encoder(ch_in=features*4,ch_out=features*8)\n",
    "        self.Conv5 = conv_block_encoder(ch_in=features*8,ch_out=features*16)\n",
    "\n",
    "\n",
    "        # Decoder\n",
    "        self.batch5 = nn.BatchNorm2d(features*16)\n",
    "        self.activation5 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.Up5 = up_conv(ch_in=features*16,ch_out=features*8)\n",
    "        self.Att5 = Attention_block(F_g=features*8,F_l=features*8,F_int=features*4)\n",
    "        self.Up_conv5 = conv_block_decoder(ch_in=features*16, ch_out=features*8)\n",
    "\n",
    "\n",
    "        self.batch4 = nn.BatchNorm2d(features*8)\n",
    "        self.activation4 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.Up4 = up_conv(ch_in=features*8,ch_out=features*4)\n",
    "        self.Att4 = Attention_block(F_g=features*4,F_l=features*4,F_int=features*2)\n",
    "        self.Up_conv4 = conv_block_decoder(ch_in=features*8, ch_out=features*4)\n",
    "\n",
    "\n",
    "        self.batch3 = nn.BatchNorm2d(features*4)\n",
    "        self.activation3 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.Up3 = up_conv(ch_in=features*4,ch_out=features*2)\n",
    "        self.Att3 = Attention_block(F_g=features*2,F_l=features*2,F_int=features*1)\n",
    "        self.Up_conv3 = conv_block_decoder(ch_in=features*4, ch_out=features*2)\n",
    "\n",
    "\n",
    "\n",
    "        self.batch2 = nn.BatchNorm2d(features*2)\n",
    "        self.activation2 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.Up2 = up_conv(ch_in=features*2,ch_out=features*1)\n",
    "        self.Att2 = Attention_block(F_g=features*1,F_l=features*1,F_int=int(features/2))\n",
    "        self.Up_conv2 = conv_block_decoder(ch_in=features*2, ch_out=features*1)\n",
    "\n",
    "        self.Conv_1x1 = nn.utils.spectral_norm(nn.Conv2d(features*1,output_ch,kernel_size=1,stride=1,padding=0))\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        # encoding path\n",
    "        x1 = self.Conv1(x)\n",
    "\n",
    "        x2 = self.Maxpool(x1)\n",
    "        x2 = self.Conv2(x2)\n",
    "\n",
    "        x3 = self.Maxpool(x2)\n",
    "        x3 = self.Conv3(x3)\n",
    "\n",
    "        x4 = self.Maxpool(x3)\n",
    "        x4 = self.Conv4(x4)\n",
    "\n",
    "        x5 = self.Maxpool(x4)\n",
    "        x5 = self.Conv5(x5)\n",
    "\n",
    "        # decoding + concat path\n",
    "        d5 = self.batch5(x5)\n",
    "        d5 = self.activation5(d5)\n",
    "        d5 = self.Up5(d5)\n",
    "        x4 = self.Att5(g=d5,x=x4)\n",
    "        d5 = torch.cat((x4,d5),dim=1)\n",
    "        d5 = self.Up_conv5(d5)\n",
    "\n",
    "        d4 = self.batch4(d5)\n",
    "        d4 = self.activation4(d4)\n",
    "        d4 = self.Up4(d4)\n",
    "        x3 = self.Att4(g=d4,x=x3)\n",
    "        d4 = torch.cat((x3,d4),dim=1)\n",
    "        d4 = self.Up_conv4(d4)\n",
    "\n",
    "        d3 = self.batch3(d4)\n",
    "        d3 = self.activation3(d3)\n",
    "        d3 = self.Up3(d3)\n",
    "        x2 = self.Att3(g=d3,x=x2)\n",
    "        d3 = torch.cat((x2,d3),dim=1)\n",
    "        d3 = self.Up_conv3(d3)\n",
    "\n",
    "        d2 = self.batch2(d3)\n",
    "        d2 = self.activation2(d2)\n",
    "        d2 = self.Up2(d2)\n",
    "        x1 = self.Att2(g=d2,x=x1)\n",
    "        d2 = torch.cat((x1,d2),dim=1)\n",
    "        d2 = self.Up_conv2(d2)\n",
    "\n",
    "        d1 = self.Conv_1x1(d2)\n",
    "\n",
    "        return d1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "(10, 128, 2, 2)"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = AttU_Net(1,1,8)\n",
    "tuple(m(torch.ones(10,1,32,32))[1].shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, img_size, latent_dim, channels):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.init_size = img_size // 4\n",
    "        self.linear_layer = nn.Sequential(nn.Linear(latent_dim, 128 * self.init_size ** 2))\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.linear_layer(z)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        image = self.conv_layers(out)\n",
    "        return image\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels, img_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            *discriminator_block(channels, 16, bn=False),\n",
    "            *discriminator_block(16, 32),\n",
    "            *discriminator_block(32, 64),\n",
    "            *discriminator_block(64, 128),\n",
    "        )\n",
    "\n",
    "        # self.ds_size = img_size // 2 ** 4 # for img_size = 96\n",
    "        self.ds_size = 2 # for img_size = 28\n",
    "        self.adverse_layer = nn.Sequential(nn.Linear(128 * self.ds_size ** 2, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, image):\n",
    "        out = self.conv_layers(image)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        validity = self.adverse_layer(out)\n",
    "        return validity"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([10, 1])"
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Discriminator(1, 32)\n",
    "m(torch.ones(10,1,28,28)).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "data": {
      "text/plain": "1049985"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(Discriminator(1, 32))\n",
    "count_parameters(Generator(32, 100, 1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Discriminator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "class DResidualBlock(nn.Module):\n",
    "    '''\n",
    "    DResidualBlock Class\n",
    "    Values:\n",
    "    in_channels: the number of channels in the input, a scalar\n",
    "    out_channels: the number of channels in the output, a scalar\n",
    "    downsample: whether to apply downsampling\n",
    "    use_preactivation: whether to apply an activation function before the first convolution\n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, downsample=True, use_preactivation=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.utils.spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        self.conv2 = nn.utils.spectral_norm(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        self.use_preactivation = use_preactivation  # apply preactivation in all except first dblock\n",
    "\n",
    "        self.downsample = downsample    # downsample occurs in all except last dblock\n",
    "        if downsample:\n",
    "            self.downsample_fn = nn.AvgPool2d(2)\n",
    "        self.mixin = (in_channels != out_channels) or downsample\n",
    "        if self.mixin:\n",
    "            self.conv_mixin = nn.utils.spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0))\n",
    "\n",
    "    def _residual(self, x):\n",
    "        if self.use_preactivation:\n",
    "            if self.mixin:\n",
    "                x = self.conv_mixin(x)\n",
    "            if self.downsample:\n",
    "                x = self.downsample_fn(x)\n",
    "        else:\n",
    "            if self.downsample:\n",
    "                x = self.downsample_fn(x)\n",
    "            if self.mixin:\n",
    "                x = self.conv_mixin(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply preactivation if applicable\n",
    "        if self.use_preactivation:\n",
    "            h = F.relu(x)\n",
    "        else:\n",
    "            h = x\n",
    "\n",
    "        h = self.conv1(h)\n",
    "        h = self.activation(h)\n",
    "        if self.downsample:\n",
    "            h = self.downsample_fn(h)\n",
    "\n",
    "        return h + self._residual(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    '''\n",
    "    AttentionBlock Class\n",
    "    Values:\n",
    "    channels: number of channels in input\n",
    "    '''\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.channels = channels\n",
    "\n",
    "        self.theta = nn.utils.spectral_norm(nn.Conv2d(channels, channels // 8, kernel_size=1, padding=0, bias=False))\n",
    "        self.phi = nn.utils.spectral_norm(nn.Conv2d(channels, channels // 8, kernel_size=1, padding=0, bias=False))\n",
    "        self.g = nn.utils.spectral_norm(nn.Conv2d(channels, channels // 2, kernel_size=1, padding=0, bias=False))\n",
    "        self.o = nn.utils.spectral_norm(nn.Conv2d(channels // 2, channels, kernel_size=1, padding=0, bias=False))\n",
    "\n",
    "        self.gamma = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        spatial_size = x.shape[2] * x.shape[3]\n",
    "\n",
    "        # Apply convolutions to get query (theta), key (phi), and value (g) transforms\n",
    "        theta = self.theta(x)\n",
    "        phi = F.max_pool2d(self.phi(x), kernel_size=2)\n",
    "        g = F.max_pool2d(self.g(x), kernel_size=2)\n",
    "\n",
    "        # Reshape spatial size for self-attention\n",
    "        theta = theta.view(-1, self.channels // 8, spatial_size)\n",
    "        phi = phi.view(-1, self.channels // 8, spatial_size // 4)\n",
    "        g = g.view(-1, self.channels // 2, spatial_size // 4)\n",
    "\n",
    "        # Compute dot product attention with query (theta) and key (phi) matrices\n",
    "        beta = F.softmax(torch.bmm(theta.transpose(1, 2), phi), dim=-1)\n",
    "\n",
    "        # Compute scaled dot product attention with value (g) and attention (beta) matrices\n",
    "        o = self.o(torch.bmm(g, beta.transpose(1, 2)).view(-1, self.channels // 2, x.shape[2], x.shape[3]))\n",
    "\n",
    "        # Apply gain and residual\n",
    "        return self.gamma * o + x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    '''\n",
    "    Discriminator Class\n",
    "    Values:\n",
    "    base_channels: the number of base channels, a scalar\n",
    "    n_classes: the number of image classes, a scalar\n",
    "    '''\n",
    "\n",
    "    def __init__(self, base_channels=96, n_classes=1000):\n",
    "        super().__init__()\n",
    "\n",
    "        # For adding class-conditional evidence\n",
    "        self.shared_emb = nn.utils.spectral_norm(nn.Embedding(n_classes, 8 * base_channels))\n",
    "\n",
    "        self.d_blocks = nn.Sequential(\n",
    "            DResidualBlock(1, base_channels, downsample=True, use_preactivation=False),\n",
    "            AttentionBlock(base_channels),\n",
    "\n",
    "            DResidualBlock(base_channels, 2 * base_channels, downsample=True, use_preactivation=True),\n",
    "            AttentionBlock(2 * base_channels),\n",
    "\n",
    "            DResidualBlock(2 * base_channels, 4 * base_channels, downsample=True, use_preactivation=True),\n",
    "            AttentionBlock(4 * base_channels),\n",
    "\n",
    "            DResidualBlock(4 * base_channels, 8 * base_channels, downsample=True, use_preactivation=True),\n",
    "            AttentionBlock(8 * base_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.proj_o = nn.utils.spectral_norm(nn.Linear(8 * base_channels, 1))\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        h = self.d_blocks(x)\n",
    "        h = torch.sum(h, dim=[2, 3])\n",
    "\n",
    "        # Class-unconditional output\n",
    "        uncond_out = self.proj_o(h)\n",
    "        if y is None:\n",
    "            return uncond_out\n",
    "\n",
    "        # Class-conditional output\n",
    "        cond_out = torch.sum(self.shared_emb(y) * h, dim=1, keepdim=True)\n",
    "        return uncond_out + cond_out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [
    {
     "data": {
      "text/plain": "83213"
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "m = Discriminator(8,1)\n",
    "x = torch.ones(10, 1, 32, 32)\n",
    "m(x).shape\n",
    "\n",
    "count_parameters(m)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nDResidualBlock(8 * base_channels, 16 * base_channels, downsample=True, use_preactivation=True),\\nAttentionBlock(16 * base_channels),\\n\\nDResidualBlock(16 * base_channels, 16 * base_channels, downsample=False, use_preactivation=True),\\nAttentionBlock(16 * base_channels),\\n'"
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agar data size bishtar dashti (128, 128) layer ha ro ziad kon ba in!!\n",
    "# NOTE:\n",
    "## havaset b \"self.proj_o \" va \"self.shared_emb\" bashe k bayad 16 shavad!!\n",
    "\n",
    "\"\"\"\n",
    "DResidualBlock(8 * base_channels, 16 * base_channels, downsample=True, use_preactivation=True),\n",
    "AttentionBlock(16 * base_channels),\n",
    "\n",
    "DResidualBlock(16 * base_channels, 16 * base_channels, downsample=False, use_preactivation=True),\n",
    "AttentionBlock(16 * base_channels),\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [
    {
     "data": {
      "text/plain": "83213"
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(Discriminator(8,1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in ./pretrain_models\\hub\\pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "os.environ['TORCH_HOME'] = './pretrain_models'\n",
    "VGG = torch.hub.load('pytorch/vision:v0.10.0', 'vgg11', pretrained=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [],
   "source": [
    "root_save = \"./best_models\"\n",
    "root_ds = \"./MNIST\"\n",
    "\n",
    "batch_size = 10\n",
    "img_size = 32\n",
    "latent_dim = 100\n",
    "channels = 1\n",
    "num_epochs = 1000\n",
    "sample_interval = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [],
   "source": [
    "trans=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.ConvertImageDtype(torch.float),\n",
    "    transforms.Normalize((0.5,) , (0.5,))\n",
    "])\n",
    "\n",
    "train_data=datasets.MNIST(root=root_ds,\n",
    "                          train=True, transform=trans, download=True)\n",
    "test_data=datasets.MNIST(root=root_ds,\n",
    "                          train=False, transform=trans, download=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [],
   "source": [
    "def get_index(class_num, dataset, mode:str):\n",
    "    if mode == 'train':\n",
    "        class_indx = torch.nonzero(\n",
    "                dataset.train_labels == class_num * torch.ones_like(dataset.train_labels)\n",
    "        )\n",
    "\n",
    "    if mode == 'test':\n",
    "        class_indx = torch.nonzero(\n",
    "                dataset.test_labels == class_num * torch.ones_like(dataset.test_labels)\n",
    "        )\n",
    "\n",
    "    class_indx = class_indx.squeeze(1)\n",
    "    return class_indx"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [],
   "source": [
    "train_sampler = SubsetRandomSampler(get_index(8, train_data, 'train'))\n",
    "test_sampler = SubsetRandomSampler(get_index(8, test_data, 'test'))\n",
    "\n",
    "train_loader=DataLoader(train_data, batch_size= batch_size, sampler=train_sampler)\n",
    "test_loader=DataLoader(test_data, batch_size= batch_size, sampler=test_sampler)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [],
   "source": [
    "#generator = Generator(img_size=img_size, latent_dim=latent_dim, channels=channels).cuda()\n",
    "generator = AttU_Net(1,1,8).cuda()\n",
    "# discriminator = Discriminator(channels=channels, img_size=img_size).cuda()\n",
    "discriminator = Discriminator(8,1).cuda()\n",
    "#VGG = VGG.cuda()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [],
   "source": [
    "adversarial_loss = nn.BCELoss()\n",
    "feature_loss = nn.MSELoss()\n",
    "optimizer_G = optim.Adam(generator.parameters())\n",
    "optimizer_D = optim.Adam(discriminator.parameters())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0.0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [],
   "source": [
    "generator = generator.apply(weights_init_normal)\n",
    "discriminator = discriminator.apply(weights_init_normal)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_3756/2675161070.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     33\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     34\u001B[0m         \u001B[0mg_loss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgan_loss\u001B[0m \u001B[1;31m#+ feature1_loss + feature2_loss + feature3_loss\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 35\u001B[1;33m         \u001B[0mg_loss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     36\u001B[0m         \u001B[0moptimizer_G\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     37\u001B[0m         \u001B[0mtotal_G_loss\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0mg_loss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcpu\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdetach\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mB:\\Anaconda_install_path\\envs\\deeplearning\\lib\\site-packages\\torch\\_tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    305\u001B[0m                 \u001B[0mcreate_graph\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    306\u001B[0m                 inputs=inputs)\n\u001B[1;32m--> 307\u001B[1;33m         \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    308\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    309\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mB:\\Anaconda_install_path\\envs\\deeplearning\\lib\\site-packages\\torch\\autograd\\__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    152\u001B[0m         \u001B[0mretain_graph\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    153\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 154\u001B[1;33m     Variable._execution_engine.run_backward(\n\u001B[0m\u001B[0;32m    155\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    156\u001B[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs):\n",
    "    total_G_loss = 0.0\n",
    "    total_D_loss = 0.0\n",
    "    for i, (real_images, _) in enumerate(train_loader):\n",
    "        valid = torch.FloatTensor(real_images.shape[0], 1).fill_(1.0).cuda()\n",
    "        fake = torch.FloatTensor(real_images.shape[0], 1).fill_(0.0).cuda()\n",
    "        real_images = real_images.cuda()\n",
    "\n",
    "        #  Train Generator\n",
    "        optimizer_G.zero_grad()\n",
    "        #z = torch.FloatTensor(np.random.normal(0, 1, (real_images.shape[0], latent_dim))).cuda()\n",
    "        z = torch.FloatTensor(np.random.normal(0, 1, (real_images.shape[0], 1, img_size, img_size))).cuda()\n",
    "        gen_imgs = generator(z)\n",
    "\n",
    "        gan_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "\n",
    "        \"\"\"\n",
    "        feature1_loss = feature_loss(\n",
    "            VGG.features[0:2](real_images.repeat(1,3,1,1)),\n",
    "            VGG.features[0:2](gen_imgs.repeat(1,3,1,1))\n",
    "        )\n",
    "\n",
    "        feature2_loss = feature_loss(\n",
    "            VGG.features[0:5](real_images.repeat(1,3,1,1)),\n",
    "            VGG.features[0:5](gen_imgs.repeat(1,3,1,1))\n",
    "        )\n",
    "\n",
    "        feature3_loss = feature_loss(\n",
    "            VGG.features[0:8](real_images.repeat(1,3,1,1)),\n",
    "            VGG.features[0:8](gen_imgs.repeat(1,3,1,1))\n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "        g_loss = gan_loss #+ feature1_loss + feature2_loss + feature3_loss\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        total_G_loss += g_loss.cpu().detach().numpy()\n",
    "\n",
    "        #  Train Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        discriminator_opinion_real = discriminator(real_images)\n",
    "        discriminator_opinion_fake = discriminator(gen_imgs.detach())\n",
    "        real_loss = adversarial_loss(discriminator_opinion_real, valid)\n",
    "        fake_loss = adversarial_loss(discriminator_opinion_fake, fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        total_D_loss += d_loss.cpu().detach().numpy()\n",
    "\n",
    "    if epoch % sample_interval ==0:\n",
    "\n",
    "        # show a sample from generator\n",
    "        #z = torch.FloatTensor(np.random.normal(0, 1, (real_images.shape[0], latent_dim))).cuda()\n",
    "        for _, (real_images, _) in enumerate(test_loader):\n",
    "\n",
    "          num = math.floor(\n",
    "                np.random.uniform(0,len(test_sampler)-100)\n",
    "            )\n",
    "          real_images = real_images[num:num+25]\n",
    "\n",
    "          z = torch.FloatTensor(np.random.normal(0, 1, (real_images.shape[0], 1, img_size, img_size))).cuda()\n",
    "          gen_imgs = generator(z)\n",
    "          show_tensor_images(gen_imgs)\n",
    "          show_tensor_images(real_images)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # show losses\n",
    "        print(\n",
    "                \"[Epoch {}/{}] \\t[D loss: {:.3f}] \\t[G loss: {:.3f}]\".format(\n",
    "                    epoch, num_epochs, total_D_loss, total_G_loss)\n",
    "            )\n",
    "\n",
    "        \"\"\"\n",
    "        print(\n",
    "                \"[Epoch {}/{}] \\t[Feature1 loss: {:.3f}] \\t[Feature2 loss: {:.3f}] \\t[Feature3 loss: {:.3f}]\".format(\n",
    "                    epoch, num_epochs, feature1_loss.item(), feature2_loss.item(), feature3_loss.item())\n",
    "            )\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        # save all\n",
    "        torch.save({\"epoch\": epoch,\n",
    "            \"state_dict_G\": generator.state_dict(),\n",
    "            \"state_dict_D\": discriminator.state_dict(),\n",
    "            \"optimizer_G\": optimizer_G.state_dict(),\n",
    "            \"optimizer_D\": optimizer_D.state_dict()\n",
    "           }, root_save + f\"/epoch{epoch}_disloss{total_D_loss}_genloss{total_G_loss}.pt\")\n",
    "        \"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### End"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}