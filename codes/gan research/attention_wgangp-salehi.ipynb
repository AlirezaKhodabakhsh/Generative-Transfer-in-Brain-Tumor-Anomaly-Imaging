{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Libs"
   ],
   "metadata": {
    "collapsed": false,
    "id": "a4MEQDsIB_3E"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torchvision.datasets import CelebA\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import math\n",
    "# main libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset\n",
    "import math\n",
    "import torchvision.transforms.functional as TF\n",
    "import cv2\n",
    "from keras.utils import image_dataset_from_directory\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "from torch.nn import init\n",
    "from torchvision.utils import make_grid\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from skimage import transform\n",
    "import skimage.io as io\n",
    "import numpy as np\n",
    "from torchvision.utils import save_image\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import vgg16\n",
    "from pathlib import Path\n",
    "from torch.autograd import Variable"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "NB4mjCjQB_3L"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Helper Functions"
   ],
   "metadata": {
    "collapsed": false,
    "id": "x3trikDlB_3N"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def show_tensor_images(image_tensor, num_images=25, size=(1, 32, 32)):\n",
    "    '''\n",
    "    Function for visualizing images: Given a tensor of images, number of images, and\n",
    "    size per image, plots and prints the images in a uniform grid.\n",
    "    '''\n",
    "    image_unflat = image_tensor.detach().cpu().view(-1, *size)\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    plt.show()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "id_J8mSjB_3O"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "G5vVTWJIB_3O"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def ploter(image, image_hat):\n",
    "    \"\"\"\n",
    "    (H, W)\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    #plt.imshow(image_hat, cmap='gray', vmin=-1, vmax=1)\n",
    "    plt.imshow(image_hat)\n",
    "    plt.tight_layout()\n",
    "    plt.title(\"Reconstruct\")\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    #plt.imshow(image, cmap='gray', vmin=-1, vmax=1)\n",
    "    plt.imshow(image)\n",
    "    plt.tight_layout()\n",
    "    plt.title(\"Original\")\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "plW9T5H9B_3P"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generator Attention"
   ],
   "metadata": {
    "collapsed": false,
    "id": "f8Q0DyE6B_3P"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class conv_block(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super(conv_block,self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=3,stride=1,padding=1,bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch_out, ch_out, kernel_size=3,stride=1,padding=1,bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        return x"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "gj4NM4uRB_3Q"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class up_conv(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super(up_conv,self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(ch_in,ch_out,kernel_size=3,stride=1,padding=1,bias=True),\n",
    "\t\t    nn.BatchNorm2d(ch_out),\n",
    "\t\t\tnn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.up(x)\n",
    "        return x"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Ma5GZ4_4B_3Q"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Attention_block(nn.Module):\n",
    "    def __init__(self,F_g,F_l,F_int):\n",
    "        super(Attention_block,self).__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "            )\n",
    "\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self,g,x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1+x1)\n",
    "        psi = self.psi(psi)\n",
    "\n",
    "        return x*psi"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "22r7A6-BB_3R"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class AttU_Net(nn.Module):\n",
    "    def __init__(self,img_ch=1,output_ch=1, features=2):\n",
    "        super(AttU_Net,self).__init__()\n",
    "\n",
    "        self.Maxpool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "\n",
    "        self.Conv1 = conv_block(ch_in=img_ch,ch_out=features*1)\n",
    "        self.Conv2 = conv_block(ch_in=features*1,ch_out=features*2)\n",
    "        self.Conv3 = conv_block(ch_in=features*2,ch_out=features*4)\n",
    "        self.Conv4 = conv_block(ch_in=features*4,ch_out=features*8)\n",
    "        self.Conv5 = conv_block(ch_in=features*8,ch_out=features*16)\n",
    "\n",
    "        self.Up5 = up_conv(ch_in=features*16,ch_out=features*8)\n",
    "        self.Att5 = Attention_block(F_g=features*8,F_l=features*8,F_int=features*4)\n",
    "        self.Up_conv5 = conv_block(ch_in=features*16, ch_out=features*8)\n",
    "\n",
    "        self.Up4 = up_conv(ch_in=features*8,ch_out=features*4)\n",
    "        self.Att4 = Attention_block(F_g=features*4,F_l=features*4,F_int=features*2)\n",
    "        self.Up_conv4 = conv_block(ch_in=features*8, ch_out=features*4)\n",
    "\n",
    "        self.Up3 = up_conv(ch_in=features*4,ch_out=features*2)\n",
    "        self.Att3 = Attention_block(F_g=features*2,F_l=features*2,F_int=features*1)\n",
    "        self.Up_conv3 = conv_block(ch_in=features*4, ch_out=features*2)\n",
    "\n",
    "        self.Up2 = up_conv(ch_in=features*2,ch_out=features*1)\n",
    "        self.Att2 = Attention_block(F_g=features*1,F_l=features*1,F_int=int(features/2))\n",
    "        self.Up_conv2 = conv_block(ch_in=features*2, ch_out=features*1)\n",
    "\n",
    "        self.Conv_1x1 = nn.Conv2d(features*1,output_ch,kernel_size=1,stride=1,padding=0)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        # encoding path\n",
    "        x1 = self.Conv1(x)\n",
    "\n",
    "        x2 = self.Maxpool(x1)\n",
    "        x2 = self.Conv2(x2)\n",
    "\n",
    "        x3 = self.Maxpool(x2)\n",
    "        x3 = self.Conv3(x3)\n",
    "\n",
    "        x4 = self.Maxpool(x3)\n",
    "        x4 = self.Conv4(x4)\n",
    "\n",
    "        x5 = self.Maxpool(x4)\n",
    "        x5 = self.Conv5(x5)\n",
    "\n",
    "        # decoding + concat path\n",
    "        d5 = self.Up5(x5)\n",
    "        x4 = self.Att5(g=d5,x=x4)\n",
    "        d5 = torch.cat((x4,d5),dim=1)\n",
    "        d5 = self.Up_conv5(d5)\n",
    "\n",
    "        d4 = self.Up4(d5)\n",
    "        x3 = self.Att4(g=d4,x=x3)\n",
    "        d4 = torch.cat((x3,d4),dim=1)\n",
    "        d4 = self.Up_conv4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4)\n",
    "        x2 = self.Att3(g=d3,x=x2)\n",
    "        d3 = torch.cat((x2,d3),dim=1)\n",
    "        d3 = self.Up_conv3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        x1 = self.Att2(g=d2,x=x1)\n",
    "        d2 = torch.cat((x1,d2),dim=1)\n",
    "        d2 = self.Up_conv2(d2)\n",
    "\n",
    "        d1 = self.Conv_1x1(d2)\n",
    "\n",
    "        return torch.tanh(d1)\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "z4ynwMx1B_3S"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "m = AttU_Net(1,1,8)\n",
    "m(torch.ones(10,1,32,32)).shape\n",
    "count_parameters(m)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NVF7GIg_B_3S",
    "outputId": "5d44aaa2-6ff2-4090-8d2c-22cfb700e6da"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Simple Generator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, img_size, latent_dim, channels):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.init_size = img_size // 4\n",
    "        self.linear_layer = nn.Sequential(nn.Linear(latent_dim, 128 * self.init_size ** 2))\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.linear_layer(z)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        image = self.conv_layers(out)\n",
    "        return image\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels, img_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            *discriminator_block(channels, 16, bn=False),\n",
    "            *discriminator_block(16, 32),\n",
    "            *discriminator_block(32, 64),\n",
    "            *discriminator_block(64, 128),\n",
    "        )\n",
    "\n",
    "        # self.ds_size = img_size // 2 ** 4 # for img_size = 96\n",
    "        self.ds_size = 2 # for img_size = 28\n",
    "        self.adverse_layer = nn.Sequential(nn.Linear(128 * self.ds_size ** 2, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, image):\n",
    "        out = self.conv_layers(image)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        validity = self.adverse_layer(out)\n",
    "        return validity"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "spYfZnR3B_3T"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Discriminator Salehi"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    '''\n",
    "    VGG model\n",
    "    '''\n",
    "\n",
    "    def __init__(self, features):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "\n",
    "        # placeholder for the gradients\n",
    "        self.gradients = None\n",
    "        self.activation = None\n",
    "\n",
    "    # hook for the gradients of the activations\n",
    "    def activations_hook(self, grad):\n",
    "        self.gradients = grad\n",
    "\n",
    "    def forward(self, x, target_layer=11):\n",
    "        result = []\n",
    "        for i in range(len(nn.ModuleList(self.features))):\n",
    "            x = self.features[i](x)\n",
    "            if i == target_layer:\n",
    "                self.activation = x\n",
    "                h = x.register_hook(self.activations_hook)\n",
    "            if i == 2 or i == 5 or i == 8 or i == 11 or i == 14 or i == 17 or i == 20 or i == 23 or i == 26 or i == 29 or i == 32 or i == 35 or i == 38:\n",
    "                result.append(x)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_activations_gradient(self):\n",
    "        return self.gradients\n",
    "\n",
    "    def get_activations(self, x):\n",
    "        return self.activation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Vgg16(torch.nn.Module):\n",
    "    def __init__(self, pretrain):\n",
    "        super(Vgg16, self).__init__()\n",
    "        features = list(vgg16('vgg16-397923af.pth').features)\n",
    "\n",
    "        if not pretrain:\n",
    "            for ind, f in enumerate(features):\n",
    "                # nn.init.xavier_normal_(f)\n",
    "                if type(f) is torch.nn.modules.conv.Conv2d:\n",
    "                    torch.nn.init.xavier_uniform(f.weight)\n",
    "                    print(\"Initialized\", ind, f)\n",
    "                else:\n",
    "                    print(\"Bypassed\", ind, f)\n",
    "            # print(\"Pre-trained Network loaded\")\n",
    "        self.features = nn.ModuleList(features).eval()\n",
    "        self.output = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = []\n",
    "        for i in range(31):\n",
    "            x = self.features[i](x)\n",
    "            if i == 1 or i == 4 or i == 6 or i == 9 or i == 11 or i == 13 or i == 16 or i == 18 or i == 20 or i == 23 or i == 25 or i == 27 or i == 30:\n",
    "                output.append(x)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_layers(cfg, use_bias, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    outputs = []\n",
    "    for i in range(len(cfg)):\n",
    "        if cfg[i] == 'O':\n",
    "            outputs.append(nn.Sequential(*layers))\n",
    "        elif cfg[i] == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, cfg[i], kernel_size=3, padding=1, bias=use_bias)\n",
    "            torch.nn.init.xavier_uniform_(conv2d.weight)\n",
    "            if batch_norm and cfg[i + 1] != 'M':\n",
    "                layers += [conv2d, nn.BatchNorm2d(cfg[i]), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = cfg[i]\n",
    "    return nn.Sequential(*layers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_arch(idx, cfg, use_bias, batch_norm=False):\n",
    "    return VGG(make_layers(cfg[idx], use_bias, batch_norm=batch_norm))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_networks(config, load_checkpoint=False):\n",
    "    equal_network_size = config['equal_network_size']\n",
    "    pretrain = config['pretrain']\n",
    "    experiment_name = config['experiment_name']\n",
    "    dataset_name = config['dataset_name']\n",
    "    normal_class = config['normal_class']\n",
    "    use_bias = config['use_bias']\n",
    "    cfg = {\n",
    "        'A': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "        'B': [16, 16, 'M', 16, 128, 'M', 16, 16, 256, 'M', 16, 16, 512, 'M', 16, 16, 512, 'M'],\n",
    "    }\n",
    "\n",
    "    if equal_network_size:\n",
    "        config_type = 'A'\n",
    "    else:\n",
    "        config_type = 'B'\n",
    "\n",
    "    vgg = Vgg16(pretrain).cuda()\n",
    "    model = make_arch(config_type, cfg, use_bias, True).cuda()\n",
    "\n",
    "    for j, item in enumerate(nn.ModuleList(model.features)):\n",
    "        print('layer : {} {}'.format(j, item))\n",
    "\n",
    "    if load_checkpoint:\n",
    "        last_checkpoint = config['last_checkpoint']\n",
    "        checkpoint_path = \"./outputs/{}/{}/checkpoints/\".format(experiment_name, dataset_name)\n",
    "        model.load_state_dict(\n",
    "            torch.load('{}Cloner_{}_epoch_{}.pth'.format(checkpoint_path, normal_class, last_checkpoint)))\n",
    "        if not pretrain:\n",
    "            vgg.load_state_dict(\n",
    "                torch.load('{}Source_{}_random_vgg.pth'.format(checkpoint_path, normal_class)))\n",
    "    elif not pretrain:\n",
    "        checkpoint_path = \"./outputs/{}/{}/checkpoints/\".format(experiment_name, dataset_name)\n",
    "        Path(checkpoint_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        torch.save(vgg.state_dict(), '{}Source_{}_random_vgg.pth'.format(checkpoint_path, normal_class))\n",
    "        print(\"Source Checkpoint saved!\")\n",
    "\n",
    "    return vgg, model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DirectionOnlyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DirectionOnlyLoss, self).__init__()\n",
    "        self.similarity_loss = torch.nn.CosineSimilarity()\n",
    "\n",
    "    def forward(self, output_pred, output_real):\n",
    "        y_pred_0, y_pred_1, y_pred_2, y_pred_3 = output_pred[3], output_pred[6], output_pred[9], output_pred[12]\n",
    "        y_0, y_1, y_2, y_3 = output_real[3], output_real[6], output_real[9], output_real[12]\n",
    "\n",
    "        loss_0 = torch.mean(1 - self.similarity_loss(y_pred_0.view(y_pred_0.shape[0], -1), y_0.view(y_0.shape[0], -1)))\n",
    "        loss_1 = torch.mean(1 - self.similarity_loss(y_pred_1.view(y_pred_1.shape[0], -1), y_1.view(y_1.shape[0], -1)))\n",
    "        loss_2 = torch.mean(1 - self.similarity_loss(y_pred_2.view(y_pred_2.shape[0], -1), y_2.view(y_2.shape[0], -1)))\n",
    "        loss_3 = torch.mean(1 - self.similarity_loss(y_pred_3.view(y_pred_3.shape[0], -1), y_3.view(y_3.shape[0], -1)))\n",
    "\n",
    "        total_loss = loss_0 + loss_1 + loss_2 + loss_3\n",
    "\n",
    "        return"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MseDirectionLoss(nn.Module):\n",
    "    def __init__(self, lamda):\n",
    "        super(MseDirectionLoss, self).__init__()\n",
    "        self.lamda = lamda\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.similarity_loss = torch.nn.CosineSimilarity()\n",
    "\n",
    "    def forward(self, output_pred, output_real):\n",
    "        y_pred_0, y_pred_1, y_pred_2, y_pred_3 = output_pred[3], output_pred[6], output_pred[9], output_pred[12]\n",
    "        y_0, y_1, y_2, y_3 = output_real[3], output_real[6], output_real[9], output_real[12]\n",
    "\n",
    "        # different terms of loss\n",
    "        abs_loss_0 = self.criterion(y_pred_0, y_0)\n",
    "        loss_0 = torch.mean(1 - self.similarity_loss(y_pred_0.view(y_pred_0.shape[0], -1), y_0.view(y_0.shape[0], -1)))\n",
    "        abs_loss_1 = self.criterion(y_pred_1, y_1)\n",
    "        loss_1 = torch.mean(1 - self.similarity_loss(y_pred_1.view(y_pred_1.shape[0], -1), y_1.view(y_1.shape[0], -1)))\n",
    "        abs_loss_2 = self.criterion(y_pred_2, y_2)\n",
    "        loss_2 = torch.mean(1 - self.similarity_loss(y_pred_2.view(y_pred_2.shape[0], -1), y_2.view(y_2.shape[0], -1)))\n",
    "        abs_loss_3 = self.criterion(y_pred_3, y_3)\n",
    "        loss_3 = torch.mean(1 - self.similarity_loss(y_pred_3.view(y_pred_3.shape[0], -1), y_3.view(y_3.shape[0], -1)))\n",
    "\n",
    "        total_loss = loss_0 + loss_1 + loss_2 + loss_3 + self.lamda * (\n",
    "                abs_loss_0 + abs_loss_1 + abs_loss_2 + abs_loss_3)\n",
    "\n",
    "        return total_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameters"
   ],
   "metadata": {
    "collapsed": false,
    "id": "_ywxVYUjB_3V"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "root_save = \"./best_models\"\n",
    "root_ds = \"./MNIST\"\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "batch_size = 64\n",
    "img_size = 32\n",
    "latent_dim = 100\n",
    "channels = 3\n",
    "num_epochs = 1000\n",
    "sample_interval = 1\n",
    "lr = 0.002\n",
    "n_critic = 5\n",
    "lambda_gp =10"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "dImR2w4YB_3V"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "id": "by7w19vYB_3V"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trans=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.ConvertImageDtype(torch.float),\n",
    "    transforms.Normalize((0.5,) , (0.5,))\n",
    "])\n",
    "\n",
    "train_data=datasets.MNIST(root=root_ds,\n",
    "                          train=True, transform=trans, download=True)\n",
    "test_data=datasets.MNIST(root=root_ds,\n",
    "                          train=False, transform=trans, download=True)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Y8z2S2LXB_3W"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_index(class_num, dataset, mode:str):\n",
    "    if mode == 'train':\n",
    "        class_indx = torch.nonzero(\n",
    "                dataset.train_labels == class_num * torch.ones_like(dataset.train_labels)\n",
    "        )\n",
    "\n",
    "    if mode == 'test':\n",
    "        class_indx = torch.nonzero(\n",
    "                dataset.test_labels == class_num * torch.ones_like(dataset.test_labels)\n",
    "        )\n",
    "\n",
    "    class_indx = class_indx.squeeze(1)\n",
    "    return class_indx"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "kn4qvuRDB_3W"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_sampler = SubsetRandomSampler(get_index(8, train_data, 'train'))\n",
    "test_sampler = SubsetRandomSampler(get_index(8, test_data, 'test'))\n",
    "\n",
    "train_loader=DataLoader(train_data, batch_size= batch_size, sampler=train_sampler)\n",
    "test_loader=DataLoader(test_data, batch_size= len(test_sampler), sampler=test_sampler)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "gRBdrBmhB_3W"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loss WGAN"
   ],
   "metadata": {
    "collapsed": false,
    "id": "3zlbPPltB_3W"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "Tensor = torch.cuda.FloatTensor # Cuda\n",
    "import torch.autograd as autograd\n",
    "\n",
    "def compute_gradient_penalty(discriminator, real_samples, fake_samples):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "    alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    d_interpolates = discriminator(interpolates)[-1].flatten(1).mean(1).unsqueeze(1)\n",
    "    fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "    # Get gradient w.r.t. interpolates\n",
    "    gradients = autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "u1CLQb7aEdRS"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### All together"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config = {'equal_network_size':False,\n",
    "'pretrain':True,\n",
    "'experiment_name':'local_equal_net',\n",
    "'dataset_name':'mnist',\n",
    "'normal_class':1,\n",
    "'use_bias':False,\n",
    "'equal_network_size':False}\n",
    "\n",
    "continue_train = False\n",
    "os.environ['TORCH_HOME'] = './pretrain_models'\n",
    "if continue_train:\n",
    "    vgg, discriminator = get_networks(config, load_checkpoint=True)\n",
    "else:\n",
    "    vgg, discriminator = get_networks(config)\n",
    "\n",
    "# Criteria And Optimizers\n",
    "direction_loss_only = False\n",
    "lamda = 0.01\n",
    "\n",
    "if direction_loss_only:\n",
    "    criterion_disc_layers = DirectionOnlyLoss()\n",
    "else:\n",
    "    criterion_disc_layers = MseDirectionLoss(lamda)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generator = Generator(img_size=img_size, latent_dim=latent_dim, channels=channels).cuda()\n",
    "#generator = AttU_Net(1,1,8).to(device)\n",
    "#discriminator = Discriminator(channels=channels, img_size=img_size).cuda()\n",
    "#discriminator = Discriminator(8,1).to(device)\n",
    "#VGG = VGG.to(device)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "jIPmH067B_3X"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "adversarial_loss = nn.BCELoss()\n",
    "feature_loss = nn.MSELoss()\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr,  betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr,  betas=(0.5, 0.999))"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "OA1uX3RAB_3X"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0.0)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "FOkqnCrOB_3X"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generator = generator.apply(weights_init_normal)\n",
    "discriminator = discriminator.apply(weights_init_normal)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "VNzaUe66B_3X"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train"
   ],
   "metadata": {
    "collapsed": false,
    "id": "lhziaqqmB_3X"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# bia va b discri ham hal bede \n",
    "\n",
    "for epoch in range(1, num_epochs):\n",
    "    total_G_loss = 0.0\n",
    "    total_D_loss = 0.0\n",
    "    for i, (real_images, _) in enumerate(train_loader):\n",
    "\n",
    "        # Real and Fake images\n",
    "        if real_images.shape[1] == 1:\n",
    "            real_images = real_images.repeat(1, 3, 1, 1)\n",
    "        real_images = Variable(real_images).to(device)\n",
    "        z = torch.FloatTensor(np.random.normal(0, 1, (real_images.shape[0], latent_dim))).to(device)\n",
    "        gen_imgs = generator(z)\n",
    "\n",
    "        ####################  Train Discriminator ####################\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        ##########  A. Adverserial Loss\n",
    "        real_validity = discriminator(real_images)[-1].flatten(1).mean(1).unsqueeze(1)\n",
    "        fake_validity = discriminator(gen_imgs.detach())[-1].flatten(1).mean(1).unsqueeze(1)\n",
    "        gradient_penalty = compute_gradient_penalty(discriminator, real_images, gen_imgs.detach())\n",
    "        d_loss_adv = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n",
    "\n",
    "        ##########  B. Layers Loss\n",
    "        d_loss_layers_real = criterion_disc_layers(discriminator.forward(real_images), vgg(real_images))\n",
    "        d_loss_layers_fake = criterion_disc_layers(discriminator.forward(gen_imgs), vgg(gen_imgs))\n",
    "        d_loss_layers = 0.5*(d_loss_layers_real + d_loss_layers_fake)\n",
    "\n",
    "        ##########  C. All Losses\n",
    "        d_loss = 0.5*(d_loss_adv + d_loss_layers)\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        total_D_loss += d_loss.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "        ####################  Train Generator ####################\n",
    "        if i % n_critic == 0:\n",
    "            # Generate a batch of images\n",
    "            z = torch.FloatTensor(np.random.normal(0, 1, (real_images.shape[0], latent_dim))).to(device)\n",
    "            gen_imgs = generator(z)\n",
    "\n",
    "            fake_validity = discriminator(gen_imgs)[-1].flatten(1).mean(1).unsqueeze(1)\n",
    "            g_loss = -torch.mean(fake_validity)\n",
    "            g_loss.backward(retain_graph=True)\n",
    "            optimizer_G.step()\n",
    "            total_G_loss += g_loss.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "\n",
    "    if epoch % sample_interval ==0:\n",
    "\n",
    "        # show a sample from generator\n",
    "        #z = torch.FloatTensor(np.random.normal(0, 1, (real_images.shape[0], latent_dim))).cuda()\n",
    "        for _, (real_images, _) in enumerate(test_loader):\n",
    "\n",
    "          num = math.floor(\n",
    "                np.random.uniform(0,len(test_sampler)-100)\n",
    "            )\n",
    "          real_images = real_images[num:num+25]\n",
    "\n",
    "          z = torch.FloatTensor(np.random.normal(0, 1, (real_images.shape[0], latent_dim))).to(device)\n",
    "          gen_imgs = generator(z)\n",
    "          show_tensor_images(gen_imgs)\n",
    "          show_tensor_images(real_images)\n",
    "\n",
    "\n",
    "\n",
    "        # show losses\n",
    "        print(\n",
    "                \"[Epoch {}/{}] \\t[D loss: {:.3f}] \\t[G loss: {:.3f}]\".format(\n",
    "                    epoch, num_epochs, total_D_loss, total_G_loss)\n",
    "            )\n",
    "\n",
    "        \"\"\"\n",
    "        print(\n",
    "                \"[Epoch {}/{}] \\t[Feature1 loss: {:.3f}] \\t[Feature2 loss: {:.3f}] \\t[Feature3 loss: {:.3f}]\".format(\n",
    "                    epoch, num_epochs, feature1_loss.item(), feature2_loss.item(), feature3_loss.item())\n",
    "            )\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        # save all\n",
    "        torch.save({\"epoch\": epoch,\n",
    "            \"state_dict_G\": generator.state_dict(),\n",
    "            \"state_dict_D\": discriminator.state_dict(),\n",
    "            \"optimizer_G\": optimizer_G.state_dict(),\n",
    "            \"optimizer_D\": optimizer_D.state_dict()\n",
    "           }, root_save + f\"/epoch{epoch}_disloss{total_D_loss}_genloss{total_G_loss}.pt\")\n",
    "        \"\"\""
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6EVo6xT7B_3Y",
    "outputId": "07dc65ec-999f-4e32-84b8-5c6fa544dd36"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "if epoch % sample_interval == 0 and i % (len(dataloader)/5) == 0:\n",
    "    save_image(gen_imgs.data[0,0],\n",
    "               \"images/{}_{}.png\".format(str(epoch).zfill(len(str(num_epochs))),\n",
    "                                         str(i).zfill(len(str(len(dataloader))))),\n",
    "               normalize=True)\n",
    "\"\"\""
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "MyoS8TaRB_3Y"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### End"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "2hMi5zqYB_3Y"
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "accelerator": "GPU",
  "gpuClass": "standard",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1e28038b1afb4b0eaa0da17c746cd5aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0506f30626734a86bea7c8a1bad56c1a",
       "IPY_MODEL_3a116436eb0e42519bbbbafcb7cdc575",
       "IPY_MODEL_5f04db43aaff45cf83c6f897714d34c3"
      ],
      "layout": "IPY_MODEL_fb8ed517f5aa48a0afa99ff0da138401"
     }
    },
    "0506f30626734a86bea7c8a1bad56c1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ae368bd2d6c64f88b52e393147bb3e33",
      "placeholder": "​",
      "style": "IPY_MODEL_e864a63019ae4170a1b1c8672fb5437d",
      "value": "100%"
     }
    },
    "3a116436eb0e42519bbbbafcb7cdc575": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2da92b47bc104e8da5cb9d5b027aa1f8",
      "max": 531460341,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f246e46c0bb5432d91712775071c96b3",
      "value": 531460341
     }
    },
    "5f04db43aaff45cf83c6f897714d34c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed189de624c1404e806385ea7b7b4a47",
      "placeholder": "​",
      "style": "IPY_MODEL_c5e165b0d520498ca9095968d4904880",
      "value": " 507M/507M [00:06&lt;00:00, 76.0MB/s]"
     }
    },
    "fb8ed517f5aa48a0afa99ff0da138401": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae368bd2d6c64f88b52e393147bb3e33": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e864a63019ae4170a1b1c8672fb5437d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2da92b47bc104e8da5cb9d5b027aa1f8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f246e46c0bb5432d91712775071c96b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ed189de624c1404e806385ea7b7b4a47": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5e165b0d520498ca9095968d4904880": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}